{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "898bb6a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T15:21:21.711924Z",
     "start_time": "2023-06-07T15:21:19.852864Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from spellchecker import SpellChecker\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9eaa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter  #apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61b8e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T15:21:35.548536Z",
     "start_time": "2023-06-07T15:21:31.288462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "#df = pd.read_csv(r'C:\\Users\\Owner\\Desktop\\Mangimind Data Science Bootcamp\\Machine Learning Project 3\\tweet_data.csv\\tweet_data.csv')\n",
    "df = pd.read_csv(r'tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed27dddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T15:21:38.089669Z",
     "start_time": "2023-06-07T15:21:35.640614Z"
    }
   },
   "outputs": [],
   "source": [
    "#Examining general information about the data\n",
    "#df.info()\n",
    "\n",
    "#Filling in the NaN values for photoURL and videoURL \n",
    "df= df. replace(np.nan,'None',regex=True)\n",
    "\n",
    "convert_dict = {'tweetID': str,\n",
    "                'crDate': 'datetime64[ns]',\n",
    "                'rtUsID': str,\n",
    "                'usID': str }\n",
    "df = df.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61804267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T15:21:39.121035Z",
     "start_time": "2023-06-07T15:21:38.177743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 785916 entries, 0 to 785915\n",
      "Data columns (total 18 columns):\n",
      " #   Column      Non-Null Count   Dtype         \n",
      "---  ------      --------------   -----         \n",
      " 0   tweetID     785916 non-null  object        \n",
      " 1   crDate      785916 non-null  datetime64[ns]\n",
      " 2   edInput     785916 non-null  int64         \n",
      " 3   editor      785916 non-null  int64         \n",
      " 4   engages     785916 non-null  int64         \n",
      " 5   isApproved  785916 non-null  bool          \n",
      " 6   isEdNeed    785916 non-null  bool          \n",
      " 7   isRT        785916 non-null  bool          \n",
      " 8   likes       785916 non-null  int64         \n",
      " 9   photoUrl    785916 non-null  object        \n",
      " 10  retweets    785916 non-null  int64         \n",
      " 11  rtUsID      785916 non-null  object        \n",
      " 12  text        785916 non-null  object        \n",
      " 13  topicName   785916 non-null  object        \n",
      " 14  usFlwrs     785916 non-null  int64         \n",
      " 15  usID        785916 non-null  object        \n",
      " 16  usName      785916 non-null  object        \n",
      " 17  videoUrl    785916 non-null  object        \n",
      "dtypes: bool(3), datetime64[ns](1), int64(6), object(8)\n",
      "memory usage: 92.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5be049f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>crDate</th>\n",
       "      <th>edInput</th>\n",
       "      <th>editor</th>\n",
       "      <th>engages</th>\n",
       "      <th>isApproved</th>\n",
       "      <th>isEdNeed</th>\n",
       "      <th>isRT</th>\n",
       "      <th>likes</th>\n",
       "      <th>photoUrl</th>\n",
       "      <th>retweets</th>\n",
       "      <th>rtUsID</th>\n",
       "      <th>text</th>\n",
       "      <th>topicName</th>\n",
       "      <th>usFlwrs</th>\n",
       "      <th>usID</th>\n",
       "      <th>usName</th>\n",
       "      <th>videoUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1070867471245164544</td>\n",
       "      <td>2018-12-07 02:27:55</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>https://pbs.twimg.com/media/Dtx8SiIWkAImVsb.jpg</td>\n",
       "      <td>34</td>\n",
       "      <td>-1</td>\n",
       "      <td>The immediate impulse for an alliance of the E...</td>\n",
       "      <td>Business</td>\n",
       "      <td>23464532</td>\n",
       "      <td>5988062</td>\n",
       "      <td>The Economist</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1070868017888837633</td>\n",
       "      <td>2018-12-07 02:30:05</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>https://pbs.twimg.com/media/Dtx8yTyW4AEciqP.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>America's economy is flashing some warning sig...</td>\n",
       "      <td>Business</td>\n",
       "      <td>1732809</td>\n",
       "      <td>16184358</td>\n",
       "      <td>CNN Business</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1070868012864028673</td>\n",
       "      <td>2018-12-07 02:30:04</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Lyft files for what is expected to be one of t...</td>\n",
       "      <td>Business</td>\n",
       "      <td>2253989</td>\n",
       "      <td>25053299</td>\n",
       "      <td>FORTUNE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1070867995239555075</td>\n",
       "      <td>2018-12-07 02:30:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Exporters still waiting to get Rs 6,000 crore ...</td>\n",
       "      <td>Business</td>\n",
       "      <td>1704056</td>\n",
       "      <td>43855487</td>\n",
       "      <td>Business Standard</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1070867995205885952</td>\n",
       "      <td>2018-12-07 02:30:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Ride-hailing firm Lyft races to leave Uber beh...</td>\n",
       "      <td>Business</td>\n",
       "      <td>1997662</td>\n",
       "      <td>15110357</td>\n",
       "      <td>Reuters Business</td>\n",
       "      <td>https://video.twimg.com/amplify_video/10708116...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweetID              crDate  edInput  editor  engages  \\\n",
       "0  1070867471245164544 2018-12-07 02:27:55       -1      -1       98   \n",
       "1  1070868017888837633 2018-12-07 02:30:05       -1      -1       13   \n",
       "2  1070868012864028673 2018-12-07 02:30:04       -1      -1       12   \n",
       "3  1070867995239555075 2018-12-07 02:30:00       -1      -1        5   \n",
       "4  1070867995205885952 2018-12-07 02:30:00       -1      -1        5   \n",
       "\n",
       "   isApproved  isEdNeed   isRT  likes  \\\n",
       "0       False      True  False     64   \n",
       "1       False      True  False     10   \n",
       "2       False      True  False      8   \n",
       "3       False      True  False      4   \n",
       "4       False      True  False      2   \n",
       "\n",
       "                                          photoUrl  retweets rtUsID  \\\n",
       "0  https://pbs.twimg.com/media/Dtx8SiIWkAImVsb.jpg        34     -1   \n",
       "1  https://pbs.twimg.com/media/Dtx8yTyW4AEciqP.jpg         3     -1   \n",
       "2                                             None         4     -1   \n",
       "3                                             None         1     -1   \n",
       "4                                             None         3     -1   \n",
       "\n",
       "                                                text topicName   usFlwrs  \\\n",
       "0  The immediate impulse for an alliance of the E...  Business  23464532   \n",
       "1  America's economy is flashing some warning sig...  Business   1732809   \n",
       "2  Lyft files for what is expected to be one of t...  Business   2253989   \n",
       "3  Exporters still waiting to get Rs 6,000 crore ...  Business   1704056   \n",
       "4  Ride-hailing firm Lyft races to leave Uber beh...  Business   1997662   \n",
       "\n",
       "       usID             usName  \\\n",
       "0   5988062      The Economist   \n",
       "1  16184358       CNN Business   \n",
       "2  25053299            FORTUNE   \n",
       "3  43855487  Business Standard   \n",
       "4  15110357   Reuters Business   \n",
       "\n",
       "                                            videoUrl  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3                                               None  \n",
       "4  https://video.twimg.com/amplify_video/10708116...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c88f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Wall Street recovers slightly from earlier losses spurred by fears of U.S.-China tensions over trade, lower oil prices https://t.co/VPvaB9zruP https://t.co/0YOxbR7cAu',\n",
       "  'New research suggests you should set harder goals\\nhttps://t.co/GwI4wT7Uzr',\n",
       "  'Judge orders U.S. State Department to disclose possible evidence whether Hillary Clinton used her private email to flout public information requests https://t.co/TTcrPnjQwD',\n",
       "  '#RajasthanElection2018 LIVE: Voting begins, Satta market bets big on @INCIndia\\n\\nCatch #live updates here\\n\\n#RajasthanAssemblyelection2018 #RajasthanElections2018 #RajasthanElections\\n\\nhttps://t.co/mzTsOMm9an',\n",
       "  \"A room full of Alpine cheese is one of Bloomberg's photos of the week üßÄüßÄüßÄhttps://t.co/22LMFT1A6S https://t.co/D6PXWArXyb\",\n",
       "  '#TelanganaElection2018 LIVE: @narendramodi , @AmitShah urge people to vote in large numbers\\n\\nCatch all the #live updates here \\n\\n #TelanganaElections #TelanganaAssemblyElections2018 #TelanganaElections2018 #Telangana\\n\\nhttps://t.co/3iNWQQx5xK',\n",
       "  \"IBM sells software assets for $1.8 billion to India's HCL https://t.co/Gs7mktaKeI\",\n",
       "  'An IPO that could value this biotech firm at an eye-popping $7.5 billion has surprised industry observers https://t.co/s5P0zCOhmC'],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's have a look at the text \n",
    "\n",
    "list(df.iloc[11:19,12]),list(df.iloc[11:19,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204ce664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before doing any text processing, please filter the twits\n",
    "df = df[(df['edInput'] == 1) | (df['edInput'] == 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4eef5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edInput\n",
       "1    215577\n",
       "2    106741\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if you have a balanced dataset\n",
    "df['edInput'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfde47b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Business\n",
       "1    0.668833\n",
       "0    0.331167\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping to derive the target variable:\n",
    "# edInput value of 1 as 1 as these tweets belong to the Business category\n",
    "# edInput value of 2 as 0 as these tweets doesnt belong to the Business category\n",
    "\n",
    "df.loc[:, 'Business'] = df['edInput'].map({1: 1, 2: 0})\n",
    "df['Business'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d81999ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, some word frequency analysis\n",
    "from collections import Counter\n",
    "def gen_word_freq_plot(tweet_type, words):\n",
    "    print(f'Number of words in {tweet_type} tweets: {len(words)}')  \n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Top 20 most frequent words in this corpus\n",
    "    n = 20\n",
    "    top_n = word_freq.most_common(n)\n",
    "    words, frequencies = zip(*top_n)  \n",
    "\n",
    "    # Plot the word frequencies\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Frequently used words in the {tweet_type} corpus')\n",
    "    sns.barplot(x=list(words), y=list(frequencies), ax=ax) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Biz_tweets = df[df['Business'] == 1].copy()\n",
    "non_Biz_tweets = df[df['Business'] == 0].copy()\n",
    "Biz_words = Biz_tweets['text'].str.split().sum()\n",
    "non_Biz_words = non_Biz_tweets['text'].str.split().sum()\n",
    "gen_word_freq_plot(\"Business\", Biz_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_word_freq_plot(\"non_Biz\", non_Biz_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can work on exploring the data a little more in depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e204972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's see some word cloud\n",
    "def gen_word_cloud(tweet_type, words):   \n",
    "    corpus_text = ' '.join(words)\n",
    "\n",
    "    # Generate word cloud from the corpus\n",
    "    wordcloud = WordCloud(width=800, \n",
    "                          height=400, \n",
    "                          background_color='white', \n",
    "                         ).generate(corpus_text)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Word Cloud in the {tweet_type} corpus')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_word_cloud('Business', Biz_words)\n",
    "gen_word_cloud('non Business', non_Biz_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_word_cloud('non Business', non_Biz_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818b31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "441b5ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4db931e7214003995742a3479c9dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/322318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>crDate</th>\n",
       "      <th>edInput</th>\n",
       "      <th>editor</th>\n",
       "      <th>engages</th>\n",
       "      <th>isApproved</th>\n",
       "      <th>isEdNeed</th>\n",
       "      <th>isRT</th>\n",
       "      <th>likes</th>\n",
       "      <th>photoUrl</th>\n",
       "      <th>retweets</th>\n",
       "      <th>rtUsID</th>\n",
       "      <th>text</th>\n",
       "      <th>topicName</th>\n",
       "      <th>usFlwrs</th>\n",
       "      <th>usID</th>\n",
       "      <th>usName</th>\n",
       "      <th>videoUrl</th>\n",
       "      <th>Business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>1070970722598707200</td>\n",
       "      <td>2018-12-07 09:18:12</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>5137</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4180</td>\n",
       "      <td>https://pbs.twimg.com/media/DtzaMK_W4AE6Vle.jpg</td>\n",
       "      <td>957</td>\n",
       "      <td>-1</td>\n",
       "      <td>Which one would you choose? üëÄ</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>37293</td>\n",
       "      <td>1061553474918342656</td>\n",
       "      <td>Gentleman‚Äôs style</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1070981773616648193</td>\n",
       "      <td>2018-12-07 10:02:07</td>\n",
       "      <td>1</td>\n",
       "      <td>5003</td>\n",
       "      <td>52650</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>36349</td>\n",
       "      <td>None</td>\n",
       "      <td>16301</td>\n",
       "      <td>781427301472874497</td>\n",
       "      <td>How it‚Äôs made. üòä</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>34897</td>\n",
       "      <td>994950431234080768</td>\n",
       "      <td>Satisfying Slime</td>\n",
       "      <td>https://video.twimg.com/ext_tw_video/107097772...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>916500037818916866</td>\n",
       "      <td>2017-10-07 03:06:51</td>\n",
       "      <td>2</td>\n",
       "      <td>5003</td>\n",
       "      <td>63872</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>39698</td>\n",
       "      <td>None</td>\n",
       "      <td>24174</td>\n",
       "      <td>821811651393495040</td>\n",
       "      <td>Smile, because it confuses people. Smile, beca...</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>966840</td>\n",
       "      <td>482658470</td>\n",
       "      <td>Tips &amp; Tricks Ideas ‚úå</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>1071135340696625152</td>\n",
       "      <td>2018-12-07 20:12:20</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>1714</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1261</td>\n",
       "      <td>https://pbs.twimg.com/media/Dt1v6O9XgAIXOrR.jpg</td>\n",
       "      <td>453</td>\n",
       "      <td>-1</td>\n",
       "      <td>ùê∂ùëôùëéùë¢ùëëùëí ùëÄùëúùëõùëíùë°</td>\n",
       "      <td>Art</td>\n",
       "      <td>33308</td>\n",
       "      <td>762649146</td>\n",
       "      <td>ùê¥ùëüùë°.</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>1071141175606829056</td>\n",
       "      <td>2018-12-07 20:35:31</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>62062</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>49073</td>\n",
       "      <td>None</td>\n",
       "      <td>12989</td>\n",
       "      <td>-1</td>\n",
       "      <td>Baby Alpacas are so under appreciated.</td>\n",
       "      <td>Animal</td>\n",
       "      <td>1052924</td>\n",
       "      <td>2828212668</td>\n",
       "      <td>Nature is Amazing ‚òòÔ∏è</td>\n",
       "      <td>https://video.twimg.com/ext_tw_video/107114109...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetID              crDate  edInput  editor  engages  \\\n",
       "721   1070970722598707200 2018-12-07 09:18:12        1    5001     5137   \n",
       "1374  1070981773616648193 2018-12-07 10:02:07        1    5003    52650   \n",
       "1867   916500037818916866 2017-10-07 03:06:51        2    5003    63872   \n",
       "2206  1071135340696625152 2018-12-07 20:12:20        1    5001     1714   \n",
       "2263  1071141175606829056 2018-12-07 20:35:31        1    5001    62062   \n",
       "\n",
       "      isApproved  isEdNeed   isRT  likes  \\\n",
       "721         True      True  False   4180   \n",
       "1374        True      True   True  36349   \n",
       "1867       False      True   True  39698   \n",
       "2206        True      True  False   1261   \n",
       "2263        True      True  False  49073   \n",
       "\n",
       "                                             photoUrl  retweets  \\\n",
       "721   https://pbs.twimg.com/media/DtzaMK_W4AE6Vle.jpg       957   \n",
       "1374                                             None     16301   \n",
       "1867                                             None     24174   \n",
       "2206  https://pbs.twimg.com/media/Dt1v6O9XgAIXOrR.jpg       453   \n",
       "2263                                             None     12989   \n",
       "\n",
       "                  rtUsID                                               text  \\\n",
       "721                   -1                     Which one would you choose? üëÄ    \n",
       "1374  781427301472874497                                  How it‚Äôs made. üòä    \n",
       "1867  821811651393495040  Smile, because it confuses people. Smile, beca...   \n",
       "2206                  -1                                      ùê∂ùëôùëéùë¢ùëëùëí ùëÄùëúùëõùëíùë°    \n",
       "2263                  -1            Baby Alpacas are so under appreciated.    \n",
       "\n",
       "        topicName  usFlwrs                 usID                 usName  \\\n",
       "721       Fashion    37293  1061553474918342656      Gentleman‚Äôs style   \n",
       "1374  Interesting    34897   994950431234080768       Satisfying Slime   \n",
       "1867  Interesting   966840            482658470  Tips & Tricks Ideas ‚úå   \n",
       "2206          Art    33308            762649146                   ùê¥ùëüùë°.   \n",
       "2263       Animal  1052924           2828212668   Nature is Amazing ‚òòÔ∏è   \n",
       "\n",
       "                                               videoUrl  Business  \n",
       "721                                                None         1  \n",
       "1374  https://video.twimg.com/ext_tw_video/107097772...         1  \n",
       "1867                                               None         0  \n",
       "2206                                               None         1  \n",
       "2263  https://video.twimg.com/ext_tw_video/107114109...         1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://t.co is shortened link from Twitter. \n",
    "# Twitter automatically shortens all the links within tweets. \n",
    "# These links doesnt provide any clue about the category of the tweet. \n",
    "# Hence we can remove them.\n",
    "\n",
    "def remove_twitter_links(text):\n",
    "    # Define a regex pattern to match the twitter urls http or https\n",
    "    twitter_url = re.compile(r'https?://t\\.co/[^\\s]+')\n",
    "\n",
    "    # Remove the urls in the text by replacing them with ''\n",
    "    text = twitter_url.sub('', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].swifter.apply(lambda x: remove_twitter_links(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbd2d7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>crDate</th>\n",
       "      <th>edInput</th>\n",
       "      <th>editor</th>\n",
       "      <th>engages</th>\n",
       "      <th>isApproved</th>\n",
       "      <th>isEdNeed</th>\n",
       "      <th>isRT</th>\n",
       "      <th>likes</th>\n",
       "      <th>photoUrl</th>\n",
       "      <th>retweets</th>\n",
       "      <th>rtUsID</th>\n",
       "      <th>text</th>\n",
       "      <th>topicName</th>\n",
       "      <th>usFlwrs</th>\n",
       "      <th>usID</th>\n",
       "      <th>usName</th>\n",
       "      <th>videoUrl</th>\n",
       "      <th>Business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>1070970722598707200</td>\n",
       "      <td>2018-12-07 09:18:12</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>5137</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4180</td>\n",
       "      <td>https://pbs.twimg.com/media/DtzaMK_W4AE6Vle.jpg</td>\n",
       "      <td>957</td>\n",
       "      <td>-1</td>\n",
       "      <td>Which one would you choose? üëÄ</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>37293</td>\n",
       "      <td>1061553474918342656</td>\n",
       "      <td>Gentleman‚Äôs style</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1070981773616648193</td>\n",
       "      <td>2018-12-07 10:02:07</td>\n",
       "      <td>1</td>\n",
       "      <td>5003</td>\n",
       "      <td>52650</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>36349</td>\n",
       "      <td>None</td>\n",
       "      <td>16301</td>\n",
       "      <td>781427301472874497</td>\n",
       "      <td>How it‚Äôs made. üòä</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>34897</td>\n",
       "      <td>994950431234080768</td>\n",
       "      <td>Satisfying Slime</td>\n",
       "      <td>https://video.twimg.com/ext_tw_video/107097772...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>916500037818916866</td>\n",
       "      <td>2017-10-07 03:06:51</td>\n",
       "      <td>2</td>\n",
       "      <td>5003</td>\n",
       "      <td>63872</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>39698</td>\n",
       "      <td>None</td>\n",
       "      <td>24174</td>\n",
       "      <td>821811651393495040</td>\n",
       "      <td>Smile, because it confuses people. Smile, beca...</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>966840</td>\n",
       "      <td>482658470</td>\n",
       "      <td>Tips &amp; Tricks Ideas ‚úå</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>1071135340696625152</td>\n",
       "      <td>2018-12-07 20:12:20</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>1714</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1261</td>\n",
       "      <td>https://pbs.twimg.com/media/Dt1v6O9XgAIXOrR.jpg</td>\n",
       "      <td>453</td>\n",
       "      <td>-1</td>\n",
       "      <td>ùê∂ùëôùëéùë¢ùëëùëí ùëÄùëúùëõùëíùë°</td>\n",
       "      <td>Art</td>\n",
       "      <td>33308</td>\n",
       "      <td>762649146</td>\n",
       "      <td>ùê¥ùëüùë°.</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>1071141175606829056</td>\n",
       "      <td>2018-12-07 20:35:31</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>62062</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>49073</td>\n",
       "      <td>None</td>\n",
       "      <td>12989</td>\n",
       "      <td>-1</td>\n",
       "      <td>Baby Alpacas are so under appreciated.</td>\n",
       "      <td>Animal</td>\n",
       "      <td>1052924</td>\n",
       "      <td>2828212668</td>\n",
       "      <td>Nature is Amazing ‚òòÔ∏è</td>\n",
       "      <td>https://video.twimg.com/ext_tw_video/107114109...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetID              crDate  edInput  editor  engages  \\\n",
       "721   1070970722598707200 2018-12-07 09:18:12        1    5001     5137   \n",
       "1374  1070981773616648193 2018-12-07 10:02:07        1    5003    52650   \n",
       "1867   916500037818916866 2017-10-07 03:06:51        2    5003    63872   \n",
       "2206  1071135340696625152 2018-12-07 20:12:20        1    5001     1714   \n",
       "2263  1071141175606829056 2018-12-07 20:35:31        1    5001    62062   \n",
       "\n",
       "      isApproved  isEdNeed   isRT  likes  \\\n",
       "721         True      True  False   4180   \n",
       "1374        True      True   True  36349   \n",
       "1867       False      True   True  39698   \n",
       "2206        True      True  False   1261   \n",
       "2263        True      True  False  49073   \n",
       "\n",
       "                                             photoUrl  retweets  \\\n",
       "721   https://pbs.twimg.com/media/DtzaMK_W4AE6Vle.jpg       957   \n",
       "1374                                             None     16301   \n",
       "1867                                             None     24174   \n",
       "2206  https://pbs.twimg.com/media/Dt1v6O9XgAIXOrR.jpg       453   \n",
       "2263                                             None     12989   \n",
       "\n",
       "                  rtUsID                                               text  \\\n",
       "721                   -1                     Which one would you choose? üëÄ    \n",
       "1374  781427301472874497                                  How it‚Äôs made. üòä    \n",
       "1867  821811651393495040  Smile, because it confuses people. Smile, beca...   \n",
       "2206                  -1                                      ùê∂ùëôùëéùë¢ùëëùëí ùëÄùëúùëõùëíùë°    \n",
       "2263                  -1            Baby Alpacas are so under appreciated.    \n",
       "\n",
       "        topicName  usFlwrs                 usID                 usName  \\\n",
       "721       Fashion    37293  1061553474918342656      Gentleman‚Äôs style   \n",
       "1374  Interesting    34897   994950431234080768       Satisfying Slime   \n",
       "1867  Interesting   966840            482658470  Tips & Tricks Ideas ‚úå   \n",
       "2206          Art    33308            762649146                   ùê¥ùëüùë°.   \n",
       "2263       Animal  1052924           2828212668   Nature is Amazing ‚òòÔ∏è   \n",
       "\n",
       "                                               videoUrl  Business  \n",
       "721                                                None         1  \n",
       "1374  https://video.twimg.com/ext_tw_video/107097772...         1  \n",
       "1867                                               None         0  \n",
       "2206                                               None         1  \n",
       "2263  https://video.twimg.com/ext_tw_video/107114109...         1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14d4c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_characters(text):\n",
    "    # Replace \\n with space character\n",
    "    cleaned_text = re.sub(r'\\n+', ' ', text)\n",
    "   \n",
    "    # Remove special characters using regex\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    # Remove training and leading spaces\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72659a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ebdf7d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T15:21:33.807Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to replace emojis and emoticons with textual descriptions\n",
    "def replace_emojis(text):\n",
    "    # Replace emojis with their textual descriptions\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove the emoji delimiters (colons)\n",
    "    text = re.sub(r':', '', text)\n",
    "    return text\n",
    "\n",
    "# # function for stemming and lemmatization ONLY USE ONE\n",
    "# def stem_and_lemmatize_tokens(tokens):\n",
    "#     stemmer = PorterStemmer()\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return [stemmer.stem(lemmatizer.lemmatize(token)) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fffba1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e1a95e302f44348c404096b3b236b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/322318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text'] = df['text'].swifter.apply(lambda x: remove_unnecessary_characters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dc595c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721                            which one would you choose\n",
       "1374                                         how its made\n",
       "1867    smile because it confuses people smile because...\n",
       "2206                                                  NaN\n",
       "2263                baby alpacas are so under appreciated\n",
       "2389                         felt cute might delete later\n",
       "2583                                   sunflower pendants\n",
       "2619                                           i need one\n",
       "2660    for some people the idea of compassion entails...\n",
       "2779                               big hop energy bferber\n",
       "2829                                  jeopardy is lit now\n",
       "3683     could you say no to that face bobbyfrenchbulldog\n",
       "3952                             m o o d whiskeyretriever\n",
       "4213                       aroma restaurant in rome italy\n",
       "4414    this is faa sai a rescue elephant she was naug...\n",
       "4507    me pulling up with all the love and affection ...\n",
       "4743             a selfrolling burrito kimcheethemaltipoo\n",
       "5395                                         winter vibes\n",
       "5467    just some tiny wrinkly bulldog puppies wrinkly...\n",
       "5613                          which one is your favourite\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e64ebe02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91c94ccc4514e79b04ec8603a83cb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/322318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text'] = df['text'].swifter.apply(lambda x: remove_unnecessary_characters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3905bce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>crDate</th>\n",
       "      <th>edInput</th>\n",
       "      <th>editor</th>\n",
       "      <th>engages</th>\n",
       "      <th>isApproved</th>\n",
       "      <th>isEdNeed</th>\n",
       "      <th>isRT</th>\n",
       "      <th>likes</th>\n",
       "      <th>photoUrl</th>\n",
       "      <th>retweets</th>\n",
       "      <th>rtUsID</th>\n",
       "      <th>text</th>\n",
       "      <th>topicName</th>\n",
       "      <th>usFlwrs</th>\n",
       "      <th>usID</th>\n",
       "      <th>usName</th>\n",
       "      <th>videoUrl</th>\n",
       "      <th>Business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>1070970722598707200</td>\n",
       "      <td>2018-12-07 09:18:12</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>5137</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4180</td>\n",
       "      <td>https://pbs.twimg.com/media/DtzaMK_W4AE6Vle.jpg</td>\n",
       "      <td>957</td>\n",
       "      <td>-1</td>\n",
       "      <td>Which one would you choose</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>37293</td>\n",
       "      <td>1061553474918342656</td>\n",
       "      <td>Gentleman‚Äôs style</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1070981773616648193</td>\n",
       "      <td>2018-12-07 10:02:07</td>\n",
       "      <td>1</td>\n",
       "      <td>5003</td>\n",
       "      <td>52650</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>36349</td>\n",
       "      <td>None</td>\n",
       "      <td>16301</td>\n",
       "      <td>781427301472874497</td>\n",
       "      <td>How its made</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>34897</td>\n",
       "      <td>994950431234080768</td>\n",
       "      <td>Satisfying Slime</td>\n",
       "      <td>https://video.twimg.com/ext_tw_video/107097772...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>916500037818916866</td>\n",
       "      <td>2017-10-07 03:06:51</td>\n",
       "      <td>2</td>\n",
       "      <td>5003</td>\n",
       "      <td>63872</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>39698</td>\n",
       "      <td>None</td>\n",
       "      <td>24174</td>\n",
       "      <td>821811651393495040</td>\n",
       "      <td>Smile because it confuses people Smile because...</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>966840</td>\n",
       "      <td>482658470</td>\n",
       "      <td>Tips &amp; Tricks Ideas ‚úå</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>1071135340696625152</td>\n",
       "      <td>2018-12-07 20:12:20</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>1714</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1261</td>\n",
       "      <td>https://pbs.twimg.com/media/Dt1v6O9XgAIXOrR.jpg</td>\n",
       "      <td>453</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>Art</td>\n",
       "      <td>33308</td>\n",
       "      <td>762649146</td>\n",
       "      <td>ùê¥ùëüùë°.</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>1071141175606829056</td>\n",
       "      <td>2018-12-07 20:35:31</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>62062</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>49073</td>\n",
       "      <td>None</td>\n",
       "      <td>12989</td>\n",
       "      <td>-1</td>\n",
       "      <td>Baby Alpacas are so under appreciated</td>\n",
       "      <td>Animal</td>\n",
       "      <td>1052924</td>\n",
       "      <td>2828212668</td>\n",
       "      <td>Nature is Amazing ‚òòÔ∏è</td>\n",
       "      <td>https://video.twimg.com/ext_tw_video/107114109...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetID              crDate  edInput  editor  engages  \\\n",
       "721   1070970722598707200 2018-12-07 09:18:12        1    5001     5137   \n",
       "1374  1070981773616648193 2018-12-07 10:02:07        1    5003    52650   \n",
       "1867   916500037818916866 2017-10-07 03:06:51        2    5003    63872   \n",
       "2206  1071135340696625152 2018-12-07 20:12:20        1    5001     1714   \n",
       "2263  1071141175606829056 2018-12-07 20:35:31        1    5001    62062   \n",
       "\n",
       "      isApproved  isEdNeed   isRT  likes  \\\n",
       "721         True      True  False   4180   \n",
       "1374        True      True   True  36349   \n",
       "1867       False      True   True  39698   \n",
       "2206        True      True  False   1261   \n",
       "2263        True      True  False  49073   \n",
       "\n",
       "                                             photoUrl  retweets  \\\n",
       "721   https://pbs.twimg.com/media/DtzaMK_W4AE6Vle.jpg       957   \n",
       "1374                                             None     16301   \n",
       "1867                                             None     24174   \n",
       "2206  https://pbs.twimg.com/media/Dt1v6O9XgAIXOrR.jpg       453   \n",
       "2263                                             None     12989   \n",
       "\n",
       "                  rtUsID                                               text  \\\n",
       "721                   -1                         Which one would you choose   \n",
       "1374  781427301472874497                                       How its made   \n",
       "1867  821811651393495040  Smile because it confuses people Smile because...   \n",
       "2206                  -1                                                      \n",
       "2263                  -1              Baby Alpacas are so under appreciated   \n",
       "\n",
       "        topicName  usFlwrs                 usID                 usName  \\\n",
       "721       Fashion    37293  1061553474918342656      Gentleman‚Äôs style   \n",
       "1374  Interesting    34897   994950431234080768       Satisfying Slime   \n",
       "1867  Interesting   966840            482658470  Tips & Tricks Ideas ‚úå   \n",
       "2206          Art    33308            762649146                   ùê¥ùëüùë°.   \n",
       "2263       Animal  1052924           2828212668   Nature is Amazing ‚òòÔ∏è   \n",
       "\n",
       "                                               videoUrl  Business  \n",
       "721                                                None         1  \n",
       "1374  https://video.twimg.com/ext_tw_video/107097772...         1  \n",
       "1867                                               None         0  \n",
       "2206                                               None         1  \n",
       "2263  https://video.twimg.com/ext_tw_video/107114109...         1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496d79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c8231ca",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T15:21:33.807Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# def lemmatize_tokens(tokens):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# def word_lemmatize(string):\n",
    "#     output = [WordNetLemmatizer().lemmatize(w) for w in string]\n",
    "#     return output\n",
    "# df['text_lemm'] = ''\n",
    "# df['text_lemm'] = df['text'].apply(word_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302261b9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T15:21:33.807Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# # Convert 'text' column to strings\n",
    "# df['text'] = df['text'].astype(str)\n",
    "\n",
    "# # Lowercasing\n",
    "# df['text'] = df['text'].str.lower()\n",
    "\n",
    "# # Removing punctuation\n",
    "# df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# # Replace emojis and emoticons with textual descriptions\n",
    "# df['text'] = df['text'].apply(replace_emojis)\n",
    "\n",
    "# # Tokenization\n",
    "# df['text'] = df['text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "\n",
    "# Apply stemming or lemmatization\n",
    "#df['text'] = df['text'].apply(lambda x: stem_tokens(x)) # or lemmatize_tokens(X)\n",
    "# df['text'] = df['text'].apply(lambda x: lemmatize_tokens(x)) # or stem_tokens(X)\n",
    "\n",
    "# # Removing stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Handling URLs\n",
    "# df['text'] = df['text'].apply(lambda x: [re.sub(r'http\\S+|www\\S+', '', word) for word in x])\n",
    "\n",
    "# # Handling mentions\n",
    "# df['text'] = df['text'].apply(lambda x: [re.sub(r'@[\\w_]+', '', word) for word in x])\n",
    "\n",
    "# # Handling hashtags\n",
    "# df['text'] = df['text'].apply(lambda x: [re.sub(r'#\\w+', '', word) for word in x])\n",
    "\n",
    "# Initialize the spell checker\n",
    "# spell = SpellChecker()\n",
    "\n",
    "# # Function to correct misspelled words in a text\n",
    "# def correct_spelling(text):\n",
    "#     # Tokenize the text into words\n",
    "#     words = text.split()\n",
    "    \n",
    "#     # Find and correct misspelled words\n",
    "#     corrected_words = []\n",
    "#     for word in words:\n",
    "#         # Check if the word is misspelled\n",
    "#         if not spell.correction(word) == word:\n",
    "#             # Correct the misspelled word\n",
    "#             corrected_word = spell.correction(word)\n",
    "#             corrected_words.append(corrected_word)\n",
    "#         else:\n",
    "#             corrected_words.append(word)\n",
    "    \n",
    "#     # Join the corrected words back into a sentence\n",
    "#     corrected_text = ' '.join(corrected_words)\n",
    "    \n",
    "#     return corrected_text\n",
    "\n",
    "# # Apply spell checking to the 'text' column\n",
    "# df['text'] = df['text'].apply(correct_spelling)\n",
    "# Print the preprocessed text column\n",
    "# print(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e4aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5ead430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07faaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e71a3be",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:07.753Z"
    }
   },
   "outputs": [],
   "source": [
    "# def lemmatize_tokens(tokens):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# df['text'] = df['text'].apply(lambda x: lemmatize_tokens(x))\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "    \n",
    "# def word_lemmatize(string):\n",
    "#     output = [WordNetLemmatizer().lemmatize(w) for w in string]\n",
    "#     return output\n",
    "\n",
    "# df['text']= df['text'].apply(word_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e9fcdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>crDate</th>\n",
       "      <th>edInput</th>\n",
       "      <th>editor</th>\n",
       "      <th>engages</th>\n",
       "      <th>isApproved</th>\n",
       "      <th>isEdNeed</th>\n",
       "      <th>isRT</th>\n",
       "      <th>likes</th>\n",
       "      <th>photoUrl</th>\n",
       "      <th>retweets</th>\n",
       "      <th>rtUsID</th>\n",
       "      <th>text</th>\n",
       "      <th>topicName</th>\n",
       "      <th>usFlwrs</th>\n",
       "      <th>usID</th>\n",
       "      <th>usName</th>\n",
       "      <th>videoUrl</th>\n",
       "      <th>Business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>1070970722598707200</td>\n",
       "      <td>2018-12-07 09:18:12</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>5137</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4180</td>\n",
       "      <td>https://pbs.twimg.com/media/DtzaMK_W4AE6Vle.jpg</td>\n",
       "      <td>957</td>\n",
       "      <td>-1</td>\n",
       "      <td>Which one would you choose</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>37293</td>\n",
       "      <td>1061553474918342656</td>\n",
       "      <td>Gentleman‚Äôs style</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1070981773616648193</td>\n",
       "      <td>2018-12-07 10:02:07</td>\n",
       "      <td>1</td>\n",
       "      <td>5003</td>\n",
       "      <td>52650</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>36349</td>\n",
       "      <td>None</td>\n",
       "      <td>16301</td>\n",
       "      <td>781427301472874497</td>\n",
       "      <td>How its made</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>34897</td>\n",
       "      <td>994950431234080768</td>\n",
       "      <td>Satisfying Slime</td>\n",
       "      <td>https://video.twimg.com/ext_tw_video/107097772...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>916500037818916866</td>\n",
       "      <td>2017-10-07 03:06:51</td>\n",
       "      <td>2</td>\n",
       "      <td>5003</td>\n",
       "      <td>63872</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>39698</td>\n",
       "      <td>None</td>\n",
       "      <td>24174</td>\n",
       "      <td>821811651393495040</td>\n",
       "      <td>Smile because it confuses people Smile because...</td>\n",
       "      <td>Interesting</td>\n",
       "      <td>966840</td>\n",
       "      <td>482658470</td>\n",
       "      <td>Tips &amp; Tricks Ideas ‚úå</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>1071135340696625152</td>\n",
       "      <td>2018-12-07 20:12:20</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>1714</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1261</td>\n",
       "      <td>https://pbs.twimg.com/media/Dt1v6O9XgAIXOrR.jpg</td>\n",
       "      <td>453</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>Art</td>\n",
       "      <td>33308</td>\n",
       "      <td>762649146</td>\n",
       "      <td>ùê¥ùëüùë°.</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>1071141175606829056</td>\n",
       "      <td>2018-12-07 20:35:31</td>\n",
       "      <td>1</td>\n",
       "      <td>5001</td>\n",
       "      <td>62062</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>49073</td>\n",
       "      <td>None</td>\n",
       "      <td>12989</td>\n",
       "      <td>-1</td>\n",
       "      <td>Baby Alpacas are so under appreciated</td>\n",
       "      <td>Animal</td>\n",
       "      <td>1052924</td>\n",
       "      <td>2828212668</td>\n",
       "      <td>Nature is Amazing ‚òòÔ∏è</td>\n",
       "      <td>https://video.twimg.com/ext_tw_video/107114109...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetID              crDate  edInput  editor  engages  \\\n",
       "721   1070970722598707200 2018-12-07 09:18:12        1    5001     5137   \n",
       "1374  1070981773616648193 2018-12-07 10:02:07        1    5003    52650   \n",
       "1867   916500037818916866 2017-10-07 03:06:51        2    5003    63872   \n",
       "2206  1071135340696625152 2018-12-07 20:12:20        1    5001     1714   \n",
       "2263  1071141175606829056 2018-12-07 20:35:31        1    5001    62062   \n",
       "\n",
       "      isApproved  isEdNeed   isRT  likes  \\\n",
       "721         True      True  False   4180   \n",
       "1374        True      True   True  36349   \n",
       "1867       False      True   True  39698   \n",
       "2206        True      True  False   1261   \n",
       "2263        True      True  False  49073   \n",
       "\n",
       "                                             photoUrl  retweets  \\\n",
       "721   https://pbs.twimg.com/media/DtzaMK_W4AE6Vle.jpg       957   \n",
       "1374                                             None     16301   \n",
       "1867                                             None     24174   \n",
       "2206  https://pbs.twimg.com/media/Dt1v6O9XgAIXOrR.jpg       453   \n",
       "2263                                             None     12989   \n",
       "\n",
       "                  rtUsID                                               text  \\\n",
       "721                   -1                         Which one would you choose   \n",
       "1374  781427301472874497                                       How its made   \n",
       "1867  821811651393495040  Smile because it confuses people Smile because...   \n",
       "2206                  -1                                                      \n",
       "2263                  -1              Baby Alpacas are so under appreciated   \n",
       "\n",
       "        topicName  usFlwrs                 usID                 usName  \\\n",
       "721       Fashion    37293  1061553474918342656      Gentleman‚Äôs style   \n",
       "1374  Interesting    34897   994950431234080768       Satisfying Slime   \n",
       "1867  Interesting   966840            482658470  Tips & Tricks Ideas ‚úå   \n",
       "2206          Art    33308            762649146                   ùê¥ùëüùë°.   \n",
       "2263       Animal  1052924           2828212668   Nature is Amazing ‚òòÔ∏è   \n",
       "\n",
       "                                               videoUrl  Business  \n",
       "721                                                None         1  \n",
       "1374  https://video.twimg.com/ext_tw_video/107097772...         1  \n",
       "1867                                               None         0  \n",
       "2206                                               None         1  \n",
       "2263  https://video.twimg.com/ext_tw_video/107114109...         1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "407ec5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51176a02d64949d1875933559b0e58de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/322318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['lem_text'] = df['text'].swifter.apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b5a0728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721                            which one would you choose\n",
       "1374                                         how its make\n",
       "1867    smile because it confuse people smile because ...\n",
       "2206                                                     \n",
       "2263                 Baby Alpacas be so under appreciated\n",
       "Name: lem_text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lem_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73c34503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>Which one would you choose</td>\n",
       "      <td>which one would you choose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>How its made</td>\n",
       "      <td>how its make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>Smile because it confuses people Smile because...</td>\n",
       "      <td>smile because it confuse people smile because ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>Baby Alpacas are so under appreciated</td>\n",
       "      <td>Baby Alpacas be so under appreciated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>Felt cute Might delete later</td>\n",
       "      <td>feel cute might delete later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>Sunflower pendants</td>\n",
       "      <td>sunflower pendant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>I need one</td>\n",
       "      <td>I need one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>For some people the idea of compassion entails...</td>\n",
       "      <td>for some people the idea of compassion entail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2779</th>\n",
       "      <td>big hop energy bferber</td>\n",
       "      <td>big hop energy bferber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>Jeopardy is lit now</td>\n",
       "      <td>Jeopardy be light now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>Could you say no to that face bobbyfrenchbulldog</td>\n",
       "      <td>could you say no to that face bobbyfrenchbulldog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3952</th>\n",
       "      <td>m o o d whiskeyretriever</td>\n",
       "      <td>m o o d whiskeyretriever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4213</th>\n",
       "      <td>Aroma Restaurant in Rome Italy</td>\n",
       "      <td>Aroma Restaurant in Rome Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>This is Faa Sai a rescue elephant She was naug...</td>\n",
       "      <td>this be Faa Sai a rescue elephant she be naugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>Me pulling up with all the love and affection ...</td>\n",
       "      <td>I pull up with all the love and affection I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>A selfrolling Burrito kimcheethemaltipoo</td>\n",
       "      <td>a selfrolle Burrito kimcheethemaltipoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>Winter vibes</td>\n",
       "      <td>winter vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>Just some tiny wrinkly bulldog puppies wrinkly...</td>\n",
       "      <td>just some tiny wrinkly bulldog puppy wrinklybu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5613</th>\n",
       "      <td>Which one is your favourite</td>\n",
       "      <td>which one be your favourite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "721                          Which one would you choose   \n",
       "1374                                       How its made   \n",
       "1867  Smile because it confuses people Smile because...   \n",
       "2206                                                      \n",
       "2263              Baby Alpacas are so under appreciated   \n",
       "2389                       Felt cute Might delete later   \n",
       "2583                                 Sunflower pendants   \n",
       "2619                                         I need one   \n",
       "2660  For some people the idea of compassion entails...   \n",
       "2779                             big hop energy bferber   \n",
       "2829                                Jeopardy is lit now   \n",
       "3683   Could you say no to that face bobbyfrenchbulldog   \n",
       "3952                           m o o d whiskeyretriever   \n",
       "4213                     Aroma Restaurant in Rome Italy   \n",
       "4414  This is Faa Sai a rescue elephant She was naug...   \n",
       "4507  Me pulling up with all the love and affection ...   \n",
       "4743           A selfrolling Burrito kimcheethemaltipoo   \n",
       "5395                                       Winter vibes   \n",
       "5467  Just some tiny wrinkly bulldog puppies wrinkly...   \n",
       "5613                        Which one is your favourite   \n",
       "\n",
       "                                               lem_text  \n",
       "721                          which one would you choose  \n",
       "1374                                       how its make  \n",
       "1867  smile because it confuse people smile because ...  \n",
       "2206                                                     \n",
       "2263               Baby Alpacas be so under appreciated  \n",
       "2389                       feel cute might delete later  \n",
       "2583                                  sunflower pendant  \n",
       "2619                                         I need one  \n",
       "2660  for some people the idea of compassion entail ...  \n",
       "2779                             big hop energy bferber  \n",
       "2829                              Jeopardy be light now  \n",
       "3683   could you say no to that face bobbyfrenchbulldog  \n",
       "3952                           m o o d whiskeyretriever  \n",
       "4213                     Aroma Restaurant in Rome Italy  \n",
       "4414  this be Faa Sai a rescue elephant she be naugh...  \n",
       "4507  I pull up with all the love and affection I ha...  \n",
       "4743             a selfrolle Burrito kimcheethemaltipoo  \n",
       "5395                                        winter vibe  \n",
       "5467  just some tiny wrinkly bulldog puppy wrinklybu...  \n",
       "5613                        which one be your favourite  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text','lem_text']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e7b3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c891db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d73f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings with NaN and drop NaN rows\n",
    "df['text'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64c96401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721                            which one would you choose\n",
       "1374                                         how its make\n",
       "1867    smile because it confuse people smile because ...\n",
       "2206                                                  NaN\n",
       "2263                 baby alpacas be so under appreciated\n",
       "2389                         feel cute might delete later\n",
       "2583                                    sunflower pendant\n",
       "2619                                           i need one\n",
       "2660    for some people the idea of compassion entail ...\n",
       "2779                               big hop energy bferber\n",
       "2829                                jeopardy be light now\n",
       "3683     could you say no to that face bobbyfrenchbulldog\n",
       "3952                             m o o d whiskeyretriever\n",
       "4213                       aroma restaurant in rome italy\n",
       "4414    this be faa sai a rescue elephant she be naugh...\n",
       "4507    i pull up with all the love and affection i ha...\n",
       "4743               a selfrolle burrito kimcheethemaltipoo\n",
       "5395                                          winter vibe\n",
       "5467    just some tiny wrinkly bulldog puppy wrinklybu...\n",
       "5613                          which one be your favourite\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7ddcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5d6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c96eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T15:16:21.938330Z",
     "start_time": "2023-06-07T15:16:21.935828Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373abcc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T15:16:25.587393Z",
     "start_time": "2023-06-07T15:16:25.569878Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5796d",
   "metadata": {},
   "source": [
    "What is baseline classifier in machine learning?\n",
    "A baseline model is essentially a simple model that acts as a reference in a machine learning project. Its main function is to contextualize the results of trained models. Baseline models usually lack complexity and may have little predictive power. Regardless, their inclusion is a necessity for many reasons.\n",
    "\n",
    "We need a baseline performance and for NLP we just tokenize without doing much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb822a",
   "metadata": {},
   "source": [
    "Removing punctuation\n",
    "This line uses the apply() method along with a lambda function to remove punctuation from each text in the 'text' column.\n",
    "The regular expression r'[^\\w\\s]' matches any non-alphanumeric and non-whitespace characters, and re.sub() replaces them with an empty string.\n",
    "\n",
    "\n",
    "Tokenization\n",
    "This line applies the word_tokenize() function from NLTK to tokenize each text in the 'text' column. \n",
    "Tokenization splits the text into individual words or tokens.\n",
    "\n",
    "\n",
    "Removing stopwords\n",
    "These lines remove stopwords from each text in the 'text' column. First, a set of stopwords for the English language is created using stopwords.words('english'). \n",
    "Then, a lambda function is applied using the apply() method, which iterates over each token in the text and keeps only the words that are not in the set of stopwords.\n",
    "\n",
    "Stemming\n",
    "These lines perform stemming on each token in the 'text' column using the Porter stemming algorithm.\n",
    "First, a PorterStemmer object is created. Then, a lambda function is applied using the apply() method, \n",
    "which iterates over each token in the text and applies the stemming algorithm to reduce each word to its base or root form.\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or root form, which is called a lemma. The lemma represents the canonical or dictionary form of a word, from which all inflected forms of the word can be generated.\n",
    "In English, lemmatization typically involves removing inflections such as plurals, verb conjugations, and adverb or adjective endings to produce the base form of the word. For example, the lemma of the word \"running\" is \"run,\" and the lemma of the word \"better\" is \"good.\"\n",
    "\n",
    "Lemmatization is commonly used in natural language processing (NLP) and text analysis tasks to normalize words and reduce vocabulary size. By reducing words to their lemmas, different forms of the same word are treated as a single token, which can improve the accuracy and efficiency of various NLP algorithms and models.\n",
    "\n",
    "Question?\n",
    "\n",
    "- Treat as separate features: If emojis and emoticons carry sentiment or additional meaning that is important for your task, you can treat them as separate features and preserve them in the text. You can encode them uniquely or represent them using special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b67e65",
   "metadata": {},
   "source": [
    "The preprocessing steps (replacing emojis and emoticons, lowercasing text, removing punctuation, tokenization, removing stopwords, applying stemming and lemmatization, handling URLs, mentions, and hashtags) are commonly used in text preprocessing for machine learning classifiers. Here's why each step is important:\n",
    "\n",
    "Replacing emojis and emoticons: Emojis and emoticons are graphical representations of emotions, and they don't carry much meaning in textual analysis. By replacing them with textual descriptions, you can convert them into meaningful words or phrases that can contribute to the understanding of the text.\n",
    "\n",
    "Lowercasing text: Lowercasing the text helps in normalization by treating different cases of the same word as identical. For example, \"apple\" and \"Apple\" will be treated as the same word.\n",
    "\n",
    "Removing punctuation: Punctuation marks don't usually contribute much to the overall meaning of the text. Removing them simplifies the text and reduces noise in the data.\n",
    "\n",
    "Tokenization: Tokenization is the process of breaking the text into individual words or tokens. It helps in further analysis and allows the model to understand the context and meaning of each word.\n",
    "\n",
    "Removing stopwords: Stopwords are common words that occur frequently in a language (e.g., \"the\", \"is\", \"and\"). These words generally don't add much value to the analysis as they are commonly used and don't carry significant meaning. Removing them helps reduce the dimensionality of the data and focuses on more important words.\n",
    "\n",
    "Applying stemming and lemmatization: Stemming and lemmatization are techniques used to reduce words to their base or root form. This helps in consolidating words with the same meaning and reducing the vocabulary size. For example, \"running,\" \"runs,\" and \"ran\" can all be stemmed to \"run.\"\n",
    "\n",
    "Handling URLs, mentions, and hashtags: URLs, mentions of usernames (e.g., \"@username\"), and hashtags (e.g., \"#example\") are specific patterns in text that can be treated differently. Handling them involves replacing URLs with a generic token, replacing mentions with a common username, and extracting the meaningful word or phrase from a hashtag.\n",
    "\n",
    "By applying these preprocessing steps, you can transform the raw text into a format that is more suitable for machine learning classifiers. It helps in reducing noise, capturing important information, and improving the performance of the classifier by focusing on relevant features.Note ONLY USE ONE\n",
    "\n",
    "Feature vectors are created for machine learning classifiers to represent the input data in a numerical format that can be processed by the algorithms. Here are the key reasons why feature vectors are important for machine learning classifiers:\n",
    "\n",
    "- Numerical Representation: Machine learning classifiers operate on numerical data. By representing the input data as feature vectors, we can leverage mathematical operations and statistical techniques that are essential for training and making predictions with classifiers.\n",
    "\n",
    "- Information Extraction: Feature vectors allow us to extract relevant information from the input data. By carefully selecting and designing the features, we can capture the key characteristics or patterns that are indicative of the class labels we want the classifier to learn and predict. These features can include numerical values, text attributes, categorical variables, or any other relevant data representations.\n",
    "\n",
    "- Model Training and Prediction: Machine learning classifiers learn patterns and relationships between the feature vectors and the corresponding class labels. During the training phase, the classifier analyzes the feature vectors to create a model that can generalize to unseen data. The feature vectors serve as the input to train the classifier, allowing it to learn the underlying patterns and make accurate predictions on new instances.\n",
    "\n",
    "- Dimensionality Reduction: Feature vectors can help in reducing the dimensionality of the input data. High-dimensional data can be computationally expensive and may suffer from the curse of dimensionality. Feature extraction or selection techniques can be applied to derive a lower-dimensional representation while preserving important information, improving the efficiency and performance of the classifier.\n",
    "\n",
    "- Standardization and Normalization: Feature vectors can be standardized or normalized to ensure consistent scales across different features. This is particularly important when features have varying units, ranges, or distributions. Standardization allows the classifier to treat all features equally, preventing certain features from dominating the learning process.\n",
    "\n",
    "- By creating appropriate feature vectors, we enable machine learning classifiers to learn from the data and make accurate predictions or classifications on new instances. The choice and design of feature vectors play a critical role in the performance and effectiveness of machine learning classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685bc83",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:07.876Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Perform one-hot encoding\n",
    "# one_hot_encoded = pd.get_dummies(df['topicName'])\n",
    "\n",
    "# # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "# df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "# # Remove the original 'topicName' column\n",
    "# df_encoded.drop('topicName', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff25044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T00:33:32.696809Z",
     "start_time": "2023-05-30T00:33:32.682793Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "One-hot encoding is commonly used for machine learning classifiers, especially when dealing with categorical variables. Many machine learning algorithms, such as logistic regression, support vector machines, and neural networks, require numerical inputs. One-hot encoding is a technique used to represent categorical variables as binary features, which can be understood and processed by these algorithms.\n",
    "\n",
    "In one-hot encoding, each category of a categorical variable is represented by a binary feature column. For a variable with n categories, n binary feature columns are created, where each column indicates whether the corresponding category is present or not. The value 1 is assigned to the column representing the category, while all other columns are filled with 0s.\n",
    "\n",
    "By performing one-hot encoding, categorical variables can be effectively incorporated into the input data, allowing the classifier to learn patterns and make predictions based on these variables. It enables the classifier to understand and utilize the information conveyed by the different categories of the variable.\n",
    "\n",
    "However, it's important to note that one-hot encoding can increase the dimensionality of the feature space, which may impact the performance of the classifier, especially if the number of categories is large. In such cases, feature selection or dimensionality reduction techniques may be applied to mitigate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0485656a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:07.919Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_encoded\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medInput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# -1 unlabled data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1 confirmed by the editor that they are business calss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2 they confirmed they are misclassified. Editor does not agree\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# eliminate three and four. Might want to included by relabling 4 as 1.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# stratification means your test splites has the same representation of all the classes. If your original has 30% of label 1 and 60 of label 2 your \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "df_encoded['edInput'].value_counts()\n",
    "# -1 unlabled data\n",
    "# 1 confirmed by the editor that they are business calss\n",
    "# 2 they confirmed they are misclassified. Editor does not agree\n",
    "# 3 editor is confused\n",
    "# 4 they are business but they were posted previously from another channel. If 4  it will noty be posted ot the user.\n",
    "# eliminate three and four. Might want to included by relabling 4 as 1.\n",
    "# stratification means your test splites has the same representation of all the classes. If your original has 30% of label 1 and 60 of label 2 your "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406339e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:07.932Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee740f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:07.948Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df = df_encoded[df_encoded['edInput'].isin([1,2])]\n",
    "filtered_df['edInput'] = filtered_df['edInput'].replace(2,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a512b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:07.965Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df['edInput'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3854bee",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:07.979Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = filtered_df['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11990165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T00:33:32.704316Z",
     "start_time": "2023-05-30T00:33:32.704316Z"
    }
   },
   "source": [
    " There are several machine learning models that are commonly used for text classification. The choice of model depends on various factors such as the size of the dataset, the complexity of the classification task, and the specific requirements of your project. Here are some popular models for text classification:\n",
    "\n",
    "    Naive Bayes: Naive Bayes is a simple and efficient probabilistic classifier. It works well with text data and is often used as a baseline model for text classification tasks.\n",
    "\n",
    "    Support Vector Machines (SVM): SVM is a powerful and versatile model for text classification. It can handle high-dimensional data and is known for its ability to find complex decision boundaries.\n",
    "\n",
    "    Random Forest: Random Forest is an ensemble model that combines multiple decision trees to make predictions. It can handle text data and is robust against overfitting.\n",
    "\n",
    "    Logistic Regression: Logistic Regression is a simple and interpretable model that works well for binary text classification tasks. It uses a logistic function to model the probability of the input belonging to a certain class.\n",
    "\n",
    "    Neural Networks: Neural Networks, especially variants like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), have shown promising results in text classification tasks. They can capture complex patterns and dependencies in the text data.\n",
    "\n",
    "The choice of model ultimately depends on the specific requirements of your project and the characteristics of your data. It's often a good idea to experiment with multiple models and compare their performance using appropriate evaluation metrics to find the best model for your text classification task.\n",
    "\n",
    "Yes, applying vectorization is typically done on the training dataset in NLP projects. Vectorization is the process of converting text data into numerical representations that can be used as input to machine learning models. This is necessary because most machine learning algorithms require numerical input.\n",
    "\n",
    "In the context of NLP, vectorization techniques such as bag-of-words (BoW), term frequency-inverse document frequency (TF-IDF), or word embeddings like Word2Vec or GloVe are commonly used. These techniques transform text data into numerical feature vectors that capture the semantic or syntactic information present in the text.\n",
    "\n",
    "When applying vectorization, it is important to fit the vectorizer (e.g., CountVectorizer or TfidfVectorizer) on the training data and then transform both the training and testing data using the fitted vectorizer. This ensures that the same vocabulary and feature representation are used consistently across the training and testing sets.\n",
    "\n",
    "By applying both CountVectorizer and TfidfVectorizer, you generate different representations of the text data. CountVectorizer represents the frequency of each word, while TfidfVectorizer represents the importance of each word in the document and the entire corpus. These different representations can be used as inputs for various NLP tasks, such as classification, clustering, or information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9d3ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T15:15:36.358576Z",
     "start_time": "2023-06-07T15:15:35.945729Z"
    }
   },
   "outputs": [],
   "source": [
    "X = filtered_df['text'].astype(str)\n",
    "y = filtered_df['edInput']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply fit_transform to the training data\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply transform to the testing data\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply fit_transform to the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply transform to the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Create an instance of LogisticRegression with increased max_iter\n",
    "logistic_regression = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Train the logistic regression model using the training data\n",
    "logistic_regression.fit(X_train_count, y_train)\n",
    "\n",
    "# Make predictions on the testing data using CountVectorizer features\n",
    "y_pred_count = logistic_regression.predict(X_test_count)\n",
    "\n",
    "# Train the logistic regression model using the training data\n",
    "logistic_regression.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the testing data using TF-IDF features\n",
    "y_pred_tfidf = logistic_regression.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the models\n",
    "accuracy_count = accuracy_score(y_test, y_pred_count)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "precision_count = precision_score(y_test, y_pred_count)\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "\n",
    "recall_count = recall_score(y_test, y_pred_count)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf)\n",
    "\n",
    "f1_count = f1_score(y_test, y_pred_count)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "print(\"CountVectorizer Accuracy: \", accuracy_count)\n",
    "print(\"TF-IDF Accuracy: \", accuracy_tfidf)\n",
    "print(\"CountVectorizer Precision: \", precision_count)\n",
    "print(\"TF-IDF Precision: \", precision_tfidf)\n",
    "print(\"CountVectorizer Recall: \", recall_count)\n",
    "print(\"TF-IDF Recall: \", recall_tfidf)\n",
    "print(\"CountVectorizer F1-score: \", f1_count)\n",
    "print(\"TF-IDF F1-score: \", f1_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69587a84",
   "metadata": {},
   "source": [
    "The conversion of elements in X_train and X_test to strings is necessary because the fit_transform and transform methods of CountVectorizer expect input data in the form of iterable over strings or bytes-like objects.\n",
    "\n",
    "In the code you provided, the original X_train and X_test are lists, and it's possible that some elements in these lists are not strings. By converting all elements to strings using str(x), we ensure that all elements in X_train and X_test are valid inputs for CountVectorizer.\n",
    "\n",
    "If your X_train and X_test already consist of strings, you can omit the conversion step and directly use them in the fit_transform and transform methods of CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab0009",
   "metadata": {},
   "source": [
    "In NLP (Natural Language Processing) projects, these evaluation metrics hold particular significance as they help assess the performance of models that deal with text data. Here's their significance in the context of NLP projects:\n",
    "\n",
    "    Accuracy: In NLP tasks, such as sentiment analysis, text classification, or spam detection, accuracy indicates the overall correctness of the model's predictions. It helps evaluate how well the model can correctly classify or predict the intended labels or categories for the given text data. High accuracy signifies that the model is making correct predictions, which is crucial for reliable results in NLP applications.\n",
    "\n",
    "   false positive - Precision: Precision is valuable in NLP projects where the focus is on avoiding false positive predictions. For example, in text classification for medical diagnosis or identifying hate speech, precision helps evaluate the model's ability to correctly classify positive instances while minimizing false positives. High precision implies that the model is better at avoiding false positives, ensuring that the predicted positive instances are indeed relevant or meaningful.\n",
    "\n",
    "    false negative - Recall: Recall is important in NLP projects where capturing all positive instances is crucial, even if it means accepting some false positives. For instance, in information retrieval tasks like search engines or document retrieval systems, high recall indicates that the model can retrieve most of the relevant documents or information, ensuring that fewer relevant instances are missed.\n",
    "\n",
    "    F1-score: F1-score combines precision and recall into a single metric, making it particularly useful when both avoiding false positives and capturing all positive instances are important. In NLP projects with imbalanced class distributions, where positive or negative instances are sparse, F1-score provides a balanced assessment of the model's performance. It helps strike a balance between precision and recall and is especially relevant when false positives and false negatives have different impacts or costs.\n",
    "\n",
    "These metrics are essential in evaluating the effectiveness and reliability of NLP models, providing insights into their performance, strengths, and weaknesses. They guide the development and improvement of NLP algorithms, helping researchers and practitioners build robust and accurate models for various NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04adf89b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:08.071Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# X = text_data\n",
    "# y = filtered_df['edInput']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(filtered_df['text'].astype(str), y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Create an instance of the TF-IDF vectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Apply the vectorizer to the training data\n",
    "# X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Apply the vectorizer to the testing data\n",
    "# X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# # Scale the TF-IDF data\n",
    "# scaler = MaxAbsScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train_tfidf)\n",
    "# X_test_scaled = scaler.transform(X_test_tfidf)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "#    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Create an instance of logistic regression\n",
    "logistic_regression = LogisticRegression(max_iter=4000)\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=logistic_regression, param_grid=param_grid, cv=5, n_jobs = -1)\n",
    "\n",
    "# Perform grid search on the training data\n",
    "grid_search.fit(X_train_count, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new instance of logistic regression with the best hyperparameters\n",
    "logistic_regression_best = LogisticRegression(**best_params)\n",
    "\n",
    "# Train the logistic regression model with the best hyperparameters\n",
    "logistic_regression_best.fit(X_train_count, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = logistic_regression_best.predict(X_train_count)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "precision = precision_score(y_train, y_pred)\n",
    "recall = recall_score(y_train, y_pred)\n",
    "f1 = f1_score(y_train, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10833fd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:08.099Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train the SVM classifier using the training data\n",
    "svm_classifier.fit(X_train_count, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = svm_classifier.predict(X_train_count)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7d63d",
   "metadata": {},
   "source": [
    "Want to see more classifiers and more nlp pipline process. in count vectorizer, you call the default parameters, there are a few paraemeters you can play with \n",
    "GridSearchCV is a class in scikit-learn that provides an automated way to perform hyperparameter tuning using grid search. Grid search is a technique that exhaustively searches the specified hyperparameter values to find the best combination of hyperparameters for a given model.\n",
    "\n",
    "GridSearchCV takes an estimator (e.g., a classifier or a regressor), a parameter grid (a dictionary specifying the hyperparameter values to search), and a cross-validation strategy as input. It then performs an exhaustive search over all possible combinations of hyperparameters specified in the parameter grid. For each combination, it trains and evaluates the model using cross-validation. The best combination of hyperparameters is determined based on a specified scoring metric.\n",
    "\n",
    "The main steps involved in using GridSearchCV are as follows:\n",
    "\n",
    "    Define the parameter grid: Specify the hyperparameters to be tuned and their corresponding values in a dictionary format.\n",
    "\n",
    "    Create an instance of the estimator: Instantiate the estimator (e.g., a classifier or a regressor) with initial hyperparameter values.\n",
    "\n",
    "    Create an instance of GridSearchCV: Pass the estimator and parameter grid as arguments to GridSearchCV, along with the desired cross-validation strategy (e.g., number of folds).\n",
    "\n",
    "    Perform grid search: Call the fit method of GridSearchCV with the training data. This will perform the grid search and find the best combination of hyperparameters.\n",
    "\n",
    "    Get the best hyperparameters: Access the best hyperparameters using the best_params_ attribute of GridSearchCV.\n",
    "\n",
    "    Create a new instance of the estimator with the best hyperparameters: Instantiate the estimator with the best hyperparameters obtained from GridSearchCV.\n",
    "\n",
    "    Train and evaluate the model: Fit the new estimator on the training data and evaluate its performance on the testing data.\n",
    "\n",
    "Using GridSearchCV helps in automating the process of hyperparameter tuning and finding the best hyperparameter values for your model. It saves you from manually trying out different combinations of hyperparameters and provides a systematic way to optimize your model's performance.\n",
    "\n",
    "param_grid is a parameter in GridSearchCV that defines the grid of hyperparameters to search. It is a dictionary where the keys represent the hyperparameter names, and the values are lists or arrays of values to be explored during the grid search.\n",
    "\n",
    "In the context of logistic regression, C, penalty, and solver are commonly used hyperparameters:\n",
    "\n",
    "    C: In logistic regression, C is the inverse of the regularization strength. It controls the trade-off between fitting the training data well and keeping the model simple to avoid overfitting. Smaller values of C result in stronger regularization, while larger values reduce the regularization effect. Typically, C is a positive float value.\n",
    "\n",
    "    penalty: The penalty hyperparameter determines the type of regularization to be applied. Regularization helps prevent overfitting by adding a penalty term to the loss function. Common options for penalty in logistic regression are 'l1' (L1 regularization, also known as Lasso) and 'l2' (L2 regularization, also known as Ridge). L1 regularization tends to produce sparse models with some coefficients set to zero, while L2 regularization encourages small weights for all features.\n",
    "\n",
    "    solver: The solver hyperparameter specifies the algorithm used for optimization during the logistic regression model fitting. Different solvers have different computational characteristics and are suitable for different types of problems. Common choices for solver include 'liblinear', 'lbfgs', 'newton-cg', 'sag', and 'saga'. The 'liblinear' solver is suitable for small-to-medium-sized problems and supports both L1 and L2 penalties.\n",
    "\n",
    "By providing different values for C, penalty, and solver in the param_grid, you can explore different combinations of hyperparameters to find the best configuration that yields the highest model performance. Grid search will systematically evaluate the model using various combinations of these hyperparameter values and identify the best combination based on the specified scoring metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182344c6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:08.123Z"
    }
   },
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f3216",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:08.141Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Fix : TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "# logistic_regression = LogisticRegression(max_iter=1000)\n",
    "# logistic_regression = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# # Create an instance of LogisticRegression\n",
    "# logistic_regression = LogisticRegression()\n",
    "\n",
    "# logistic_regression = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# # Train the logistic regression model using the training data\n",
    "# logistic_regression.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Accuracy: \" + str(accuracy))\n",
    "# print(\"Precision: \" + str(precision))\n",
    "# print(\"Recall: \" + str(recall))\n",
    "# print(\"F1-score: \" + str(f1))\n",
    "\n",
    "# #next step\n",
    "# # drop one hot encodded columns, only keep the text column. \n",
    "# # our clssifier should only predict bussiness class or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e91da",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:08.162Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(bow_features, df_encoded['Business'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # The purpose of this code is to extract different types of text features, such as Bag-of-Words, TF-IDF representations, \n",
    "# # and word embeddings, from the preprocessed text data. \n",
    "# # These features can be used as inputs for various machine learning or natural language processing tasks. \n",
    "# # The code ensures that feature extraction is performed only when there are valid documents available for analysis.\n",
    "# # Convert the elements in the 'text' column to strings\n",
    "# text_data = [' '.join(text) for text in df['text']]\n",
    "\n",
    "# # Create an instance of CountVectorizer\n",
    "# bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the text data\n",
    "# bow_features = bow_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# # # Print bow_features\n",
    "# # print(bow_features)\n",
    "\n",
    "# # Create an instance of TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the text data\n",
    "# tfidf_features = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# # Print tfidf_features\n",
    "# print(tfidf_features)\n",
    "\n",
    "# # # Word embeddings\n",
    "# # word2vec_model = gensim.models.Word2Vec(sentences=df['text'], vector_size=100, min_count=1)\n",
    "\n",
    "# # for text in df['text']:\n",
    "# #      word_embeddings = word2vec_model.wv[text]\n",
    "# #      print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6a292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee733d90",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:08.197Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Convert the elements in the 'text' column to strings\n",
    "# text_data = [' '.join(text) for text in filtered_df['text']]\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(text_data, filtered_df['edInput'], test_size=0.2, random_state=42, stratify= filtered_df['edInput']  )\n",
    "\n",
    "# # Create an instance of CountVectorizer\n",
    "# count_vectorizer = CountVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the training data\n",
    "# X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Apply transform to the testing data\n",
    "# X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# # Create an instance of TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the training data\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Apply transform to the testing data\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5527ab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-07T14:42:08.219Z"
    }
   },
   "outputs": [],
   "source": [
    "# X = filtered_df['text'].astype(str)\n",
    "# y = filtered_df['edInput']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Create an instance of CountVectorizer\n",
    "# count_vectorizer = CountVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the training data\n",
    "# X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Apply transform to the testing data\n",
    "# X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# # Create an instance of LogisticRegression with increased max_iter\n",
    "# logistic_regression = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# # Train the logistic regression model using the training data\n",
    "# logistic_regression.fit(X_train_count, y_train)\n",
    "\n",
    "# # Make predictions on the testing data using CountVectorizer features\n",
    "# y_pred_count = logistic_regression.predict(X_test_count)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy_count = accuracy_score(y_test, y_pred_count)\n",
    "# precision_count = precision_score(y_test, y_pred_count)\n",
    "# recall_count = recall_score(y_test, y_pred_count)\n",
    "# f1_count = f1_score(y_test, y_pred_count)\n",
    "\n",
    "# print(\"CountVectorizer Accuracy: \", accuracy_count)\n",
    "# print(\"CountVectorizer Precision: \", precision_count)\n",
    "# print(\"CountVectorizer Recall: \", recall_count)\n",
    "# print(\"CountVectorizer F1-score: \", f1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8ab11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5309073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd894a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79b29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
