{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898bb6a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:27:36.116106Z",
     "start_time": "2023-05-30T12:27:36.097589Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61b8e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:13:34.819091Z",
     "start_time": "2023-05-30T12:13:31.827230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\Owner\\Desktop\\Mangimind Data Science Bootcamp\\Machine Learning Project 3\\tweet_data.csv\\tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed27dddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:13:36.215257Z",
     "start_time": "2023-05-30T12:13:34.820592Z"
    }
   },
   "outputs": [],
   "source": [
    "#Examining general information about the data\n",
    "#df.info()\n",
    "\n",
    "#Filling in the NaN values for photoURL and videoURL \n",
    "df= df. replace(np.nan,'None',regex=True)\n",
    "\n",
    "convert_dict = {'tweetID': str,\n",
    "                'crDate': 'datetime64[ns]',\n",
    "                'rtUsID': str,\n",
    "                'usID': str }\n",
    "df = df.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61804267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:13:36.726919Z",
     "start_time": "2023-05-30T12:13:36.216758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 785916 entries, 0 to 785915\n",
      "Data columns (total 18 columns):\n",
      " #   Column      Non-Null Count   Dtype         \n",
      "---  ------      --------------   -----         \n",
      " 0   tweetID     785916 non-null  object        \n",
      " 1   crDate      785916 non-null  datetime64[ns]\n",
      " 2   edInput     785916 non-null  int64         \n",
      " 3   editor      785916 non-null  int64         \n",
      " 4   engages     785916 non-null  int64         \n",
      " 5   isApproved  785916 non-null  bool          \n",
      " 6   isEdNeed    785916 non-null  bool          \n",
      " 7   isRT        785916 non-null  bool          \n",
      " 8   likes       785916 non-null  int64         \n",
      " 9   photoUrl    785916 non-null  object        \n",
      " 10  retweets    785916 non-null  int64         \n",
      " 11  rtUsID      785916 non-null  object        \n",
      " 12  text        785916 non-null  object        \n",
      " 13  topicName   785916 non-null  object        \n",
      " 14  usFlwrs     785916 non-null  int64         \n",
      " 15  usID        785916 non-null  object        \n",
      " 16  usName      785916 non-null  object        \n",
      " 17  videoUrl    785916 non-null  object        \n",
      "dtypes: bool(3), datetime64[ns](1), int64(6), object(8)\n",
      "memory usage: 92.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302261b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:15:50.295903Z",
     "start_time": "2023-05-30T12:13:36.727920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to replace emojis and emoticons with textual descriptions\n",
    "def replace_emojis(text):\n",
    "    # Replace emojis with their textual descriptions\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove the emoji delimiters (colons)\n",
    "    text = re.sub(r':', '', text)\n",
    "    return text\n",
    "\n",
    "# # function for stemming and lemmatization ONLY USE ONE\n",
    "# def stem_and_lemmatize_tokens(tokens):\n",
    "#     stemmer = PorterStemmer()\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return [stemmer.stem(lemmatizer.lemmatize(token)) for token in tokens]\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Convert 'text' column to strings\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Lowercasing\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# Removing punctuation\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Replace emojis and emoticons with textual descriptions\n",
    "df['text'] = df['text'].apply(replace_emojis)\n",
    "\n",
    "# Tokenization\n",
    "df['text'] = df['text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "\n",
    "# Apply stemming or lemmatization\n",
    "#df['text'] = df['text'].apply(lambda x: stem_tokens(x)) # or lemmatize_tokens(X)\n",
    "df['text'] = df['text'].apply(lambda x: lemmatize_tokens(x)) # or stem_tokens(X)\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Handling URLs\n",
    "df['text'] = df['text'].apply(lambda x: [re.sub(r'http\\S+|www\\S+', 'URL', word) for word in x])\n",
    "\n",
    "# Handling mentions\n",
    "df['text'] = df['text'].apply(lambda x: [re.sub(r'@[\\w_]+', '', word) for word in x])\n",
    "\n",
    "# Handling hashtags\n",
    "df['text'] = df['text'].apply(lambda x: [re.sub(r'#\\w+', '', word) for word in x])\n",
    "\n",
    "# Print the preprocessed text column\n",
    "# print(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5796d",
   "metadata": {},
   "source": [
    "What is baseline classifier in machine learning?\n",
    "A baseline model is essentially a simple model that acts as a reference in a machine learning project. Its main function is to contextualize the results of trained models. Baseline models usually lack complexity and may have little predictive power. Regardless, their inclusion is a necessity for many reasons.\n",
    "\n",
    "We need a baseline performance and for NLP we just tokenize without doing much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb822a",
   "metadata": {},
   "source": [
    "Removing punctuation\n",
    "This line uses the apply() method along with a lambda function to remove punctuation from each text in the 'text' column.\n",
    "The regular expression r'[^\\w\\s]' matches any non-alphanumeric and non-whitespace characters, and re.sub() replaces them with an empty string.\n",
    "\n",
    "\n",
    "Tokenization\n",
    "This line applies the word_tokenize() function from NLTK to tokenize each text in the 'text' column. \n",
    "Tokenization splits the text into individual words or tokens.\n",
    "\n",
    "\n",
    "Removing stopwords\n",
    "These lines remove stopwords from each text in the 'text' column. First, a set of stopwords for the English language is created using stopwords.words('english'). \n",
    "Then, a lambda function is applied using the apply() method, which iterates over each token in the text and keeps only the words that are not in the set of stopwords.\n",
    "\n",
    "Stemming\n",
    "These lines perform stemming on each token in the 'text' column using the Porter stemming algorithm.\n",
    "First, a PorterStemmer object is created. Then, a lambda function is applied using the apply() method, \n",
    "which iterates over each token in the text and applies the stemming algorithm to reduce each word to its base or root form.\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or root form, which is called a lemma. The lemma represents the canonical or dictionary form of a word, from which all inflected forms of the word can be generated.\n",
    "In English, lemmatization typically involves removing inflections such as plurals, verb conjugations, and adverb or adjective endings to produce the base form of the word. For example, the lemma of the word \"running\" is \"run,\" and the lemma of the word \"better\" is \"good.\"\n",
    "\n",
    "Lemmatization is commonly used in natural language processing (NLP) and text analysis tasks to normalize words and reduce vocabulary size. By reducing words to their lemmas, different forms of the same word are treated as a single token, which can improve the accuracy and efficiency of various NLP algorithms and models.\n",
    "\n",
    "Question?\n",
    "\n",
    "- Treat as separate features: If emojis and emoticons carry sentiment or additional meaning that is important for your task, you can treat them as separate features and preserve them in the text. You can encode them uniquely or represent them using special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b67e65",
   "metadata": {},
   "source": [
    "The preprocessing steps (replacing emojis and emoticons, lowercasing text, removing punctuation, tokenization, removing stopwords, applying stemming and lemmatization, handling URLs, mentions, and hashtags) are commonly used in text preprocessing for machine learning classifiers. Here's why each step is important:\n",
    "\n",
    "Replacing emojis and emoticons: Emojis and emoticons are graphical representations of emotions, and they don't carry much meaning in textual analysis. By replacing them with textual descriptions, you can convert them into meaningful words or phrases that can contribute to the understanding of the text.\n",
    "\n",
    "Lowercasing text: Lowercasing the text helps in normalization by treating different cases of the same word as identical. For example, \"apple\" and \"Apple\" will be treated as the same word.\n",
    "\n",
    "Removing punctuation: Punctuation marks don't usually contribute much to the overall meaning of the text. Removing them simplifies the text and reduces noise in the data.\n",
    "\n",
    "Tokenization: Tokenization is the process of breaking the text into individual words or tokens. It helps in further analysis and allows the model to understand the context and meaning of each word.\n",
    "\n",
    "Removing stopwords: Stopwords are common words that occur frequently in a language (e.g., \"the\", \"is\", \"and\"). These words generally don't add much value to the analysis as they are commonly used and don't carry significant meaning. Removing them helps reduce the dimensionality of the data and focuses on more important words.\n",
    "\n",
    "Applying stemming and lemmatization: Stemming and lemmatization are techniques used to reduce words to their base or root form. This helps in consolidating words with the same meaning and reducing the vocabulary size. For example, \"running,\" \"runs,\" and \"ran\" can all be stemmed to \"run.\"\n",
    "\n",
    "Handling URLs, mentions, and hashtags: URLs, mentions of usernames (e.g., \"@username\"), and hashtags (e.g., \"#example\") are specific patterns in text that can be treated differently. Handling them involves replacing URLs with a generic token, replacing mentions with a common username, and extracting the meaningful word or phrase from a hashtag.\n",
    "\n",
    "By applying these preprocessing steps, you can transform the raw text into a format that is more suitable for machine learning classifiers. It helps in reducing noise, capturing important information, and improving the performance of the classifier by focusing on relevant features.Note ONLY USE ONE\n",
    "\n",
    "Feature vectors are created for machine learning classifiers to represent the input data in a numerical format that can be processed by the algorithms. Here are the key reasons why feature vectors are important for machine learning classifiers:\n",
    "\n",
    "- Numerical Representation: Machine learning classifiers operate on numerical data. By representing the input data as feature vectors, we can leverage mathematical operations and statistical techniques that are essential for training and making predictions with classifiers.\n",
    "\n",
    "- Information Extraction: Feature vectors allow us to extract relevant information from the input data. By carefully selecting and designing the features, we can capture the key characteristics or patterns that are indicative of the class labels we want the classifier to learn and predict. These features can include numerical values, text attributes, categorical variables, or any other relevant data representations.\n",
    "\n",
    "- Model Training and Prediction: Machine learning classifiers learn patterns and relationships between the feature vectors and the corresponding class labels. During the training phase, the classifier analyzes the feature vectors to create a model that can generalize to unseen data. The feature vectors serve as the input to train the classifier, allowing it to learn the underlying patterns and make accurate predictions on new instances.\n",
    "\n",
    "- Dimensionality Reduction: Feature vectors can help in reducing the dimensionality of the input data. High-dimensional data can be computationally expensive and may suffer from the curse of dimensionality. Feature extraction or selection techniques can be applied to derive a lower-dimensional representation while preserving important information, improving the efficiency and performance of the classifier.\n",
    "\n",
    "- Standardization and Normalization: Feature vectors can be standardized or normalized to ensure consistent scales across different features. This is particularly important when features have varying units, ranges, or distributions. Standardization allows the classifier to treat all features equally, preventing certain features from dominating the learning process.\n",
    "\n",
    "- By creating appropriate feature vectors, we enable machine learning classifiers to learn from the data and make accurate predictions or classifications on new instances. The choice and design of feature vectors play a critical role in the performance and effectiveness of machine learning classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4685bc83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:15:50.621480Z",
     "start_time": "2023-05-30T12:15:50.296904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(df['topicName'])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "# Remove the original 'topicName' column\n",
    "df_encoded.drop('topicName', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff25044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T00:33:32.696809Z",
     "start_time": "2023-05-30T00:33:32.682793Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "One-hot encoding is commonly used for machine learning classifiers, especially when dealing with categorical variables. Many machine learning algorithms, such as logistic regression, support vector machines, and neural networks, require numerical inputs. One-hot encoding is a technique used to represent categorical variables as binary features, which can be understood and processed by these algorithms.\n",
    "\n",
    "In one-hot encoding, each category of a categorical variable is represented by a binary feature column. For a variable with n categories, n binary feature columns are created, where each column indicates whether the corresponding category is present or not. The value 1 is assigned to the column representing the category, while all other columns are filled with 0s.\n",
    "\n",
    "By performing one-hot encoding, categorical variables can be effectively incorporated into the input data, allowing the classifier to learn patterns and make predictions based on these variables. It enables the classifier to understand and utilize the information conveyed by the different categories of the variable.\n",
    "\n",
    "However, it's important to note that one-hot encoding can increase the dimensionality of the feature space, which may impact the performance of the classifier, especially if the number of categories is large. In such cases, feature selection or dimensionality reduction techniques may be applied to mitigate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27e91da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:15:50.636803Z",
     "start_time": "2023-05-30T12:15:50.622481Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(bow_features, df_encoded['Business'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # The purpose of this code is to extract different types of text features, such as Bag-of-Words, TF-IDF representations, \n",
    "# # and word embeddings, from the preprocessed text data. \n",
    "# # These features can be used as inputs for various machine learning or natural language processing tasks. \n",
    "# # The code ensures that feature extraction is performed only when there are valid documents available for analysis.\n",
    "# # Convert the elements in the 'text' column to strings\n",
    "# text_data = [' '.join(text) for text in df['text']]\n",
    "\n",
    "# # Create an instance of CountVectorizer\n",
    "# bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the text data\n",
    "# bow_features = bow_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# # # Print bow_features\n",
    "# # print(bow_features)\n",
    "\n",
    "# # Create an instance of TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the text data\n",
    "# tfidf_features = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# # Print tfidf_features\n",
    "# print(tfidf_features)\n",
    "\n",
    "# # # Word embeddings\n",
    "# # word2vec_model = gensim.models.Word2Vec(sentences=df['text'], vector_size=100, min_count=1)\n",
    "\n",
    "# # for text in df['text']:\n",
    "# #      word_embeddings = word2vec_model.wv[text]\n",
    "# #      print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee733d90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:20:04.047934Z",
     "start_time": "2023-05-30T12:19:50.718025Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the elements in the 'text' column to strings\n",
    "text_data = [' '.join(text) for text in df['text']]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df_encoded['Business'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply fit_transform to the training data\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply transform to the testing data\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply fit_transform to the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply transform to the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11990165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T00:33:32.704316Z",
     "start_time": "2023-05-30T00:33:32.704316Z"
    }
   },
   "source": [
    " There are several machine learning models that are commonly used for text classification. The choice of model depends on various factors such as the size of the dataset, the complexity of the classification task, and the specific requirements of your project. Here are some popular models for text classification:\n",
    "\n",
    "    Naive Bayes: Naive Bayes is a simple and efficient probabilistic classifier. It works well with text data and is often used as a baseline model for text classification tasks.\n",
    "\n",
    "    Support Vector Machines (SVM): SVM is a powerful and versatile model for text classification. It can handle high-dimensional data and is known for its ability to find complex decision boundaries.\n",
    "\n",
    "    Random Forest: Random Forest is an ensemble model that combines multiple decision trees to make predictions. It can handle text data and is robust against overfitting.\n",
    "\n",
    "    Logistic Regression: Logistic Regression is a simple and interpretable model that works well for binary text classification tasks. It uses a logistic function to model the probability of the input belonging to a certain class.\n",
    "\n",
    "    Neural Networks: Neural Networks, especially variants like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), have shown promising results in text classification tasks. They can capture complex patterns and dependencies in the text data.\n",
    "\n",
    "The choice of model ultimately depends on the specific requirements of your project and the characteristics of your data. It's often a good idea to experiment with multiple models and compare their performance using appropriate evaluation metrics to find the best model for your text classification task.\n",
    "\n",
    "Yes, applying vectorization is typically done on the training dataset in NLP projects. Vectorization is the process of converting text data into numerical representations that can be used as input to machine learning models. This is necessary because most machine learning algorithms require numerical input.\n",
    "\n",
    "In the context of NLP, vectorization techniques such as bag-of-words (BoW), term frequency-inverse document frequency (TF-IDF), or word embeddings like Word2Vec or GloVe are commonly used. These techniques transform text data into numerical feature vectors that capture the semantic or syntactic information present in the text.\n",
    "\n",
    "When applying vectorization, it is important to fit the vectorizer (e.g., CountVectorizer or TfidfVectorizer) on the training data and then transform both the training and testing data using the fitted vectorizer. This ensures that the same vocabulary and feature representation are used consistently across the training and testing sets.\n",
    "\n",
    "By applying both CountVectorizer and TfidfVectorizer, you generate different representations of the text data. CountVectorizer represents the frequency of each word, while TfidfVectorizer represents the importance of each word in the document and the entire corpus. These different representations can be used as inputs for various NLP tasks, such as classification, clustering, or information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d5527ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:28:59.313368Z",
     "start_time": "2023-05-30T12:27:51.628946Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Accuracy:  0.8858471600162866\n",
      "TF-IDF Accuracy:  0.8827679662052117\n",
      "CountVectorizer Precision:  0.7837301587301587\n",
      "TF-IDF Precision:  0.7852045670789725\n",
      "CountVectorizer Recall:  0.6259332622276398\n",
      "TF-IDF Recall:  0.6035654426329422\n",
      "CountVectorizer F1-score:  0.6959998644596175\n",
      "TF-IDF F1-score:  0.6825065903961129\n"
     ]
    }
   ],
   "source": [
    "X = text_data\n",
    "y = df_encoded['Business']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply fit_transform to the training data\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply transform to the testing data\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply fit_transform to the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply transform to the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Create an instance of LogisticRegression with increased max_iter\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the logistic regression model using the training data\n",
    "logistic_regression.fit(X_train_count, y_train)\n",
    "\n",
    "# Make predictions on the testing data using CountVectorizer features\n",
    "y_pred_count = logistic_regression.predict(X_test_count)\n",
    "\n",
    "# Train the logistic regression model using the training data\n",
    "logistic_regression.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the testing data using TF-IDF features\n",
    "y_pred_tfidf = logistic_regression.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the models\n",
    "accuracy_count = accuracy_score(y_test, y_pred_count)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "precision_count = precision_score(y_test, y_pred_count)\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "\n",
    "recall_count = recall_score(y_test, y_pred_count)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf)\n",
    "\n",
    "f1_count = f1_score(y_test, y_pred_count)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "print(\"CountVectorizer Accuracy: \", accuracy_count)\n",
    "print(\"TF-IDF Accuracy: \", accuracy_tfidf)\n",
    "print(\"CountVectorizer Precision: \", precision_count)\n",
    "print(\"TF-IDF Precision: \", precision_tfidf)\n",
    "print(\"CountVectorizer Recall: \", recall_count)\n",
    "print(\"TF-IDF Recall: \", recall_tfidf)\n",
    "print(\"CountVectorizer F1-score: \", f1_count)\n",
    "print(\"TF-IDF F1-score: \", f1_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab0009",
   "metadata": {},
   "source": [
    "In NLP (Natural Language Processing) projects, these evaluation metrics hold particular significance as they help assess the performance of models that deal with text data. Here's their significance in the context of NLP projects:\n",
    "\n",
    "    Accuracy: In NLP tasks, such as sentiment analysis, text classification, or spam detection, accuracy indicates the overall correctness of the model's predictions. It helps evaluate how well the model can correctly classify or predict the intended labels or categories for the given text data. High accuracy signifies that the model is making correct predictions, which is crucial for reliable results in NLP applications.\n",
    "\n",
    "    Precision: Precision is valuable in NLP projects where the focus is on avoiding false positive predictions. For example, in text classification for medical diagnosis or identifying hate speech, precision helps evaluate the model's ability to correctly classify positive instances while minimizing false positives. High precision implies that the model is better at avoiding false positives, ensuring that the predicted positive instances are indeed relevant or meaningful.\n",
    "\n",
    "    Recall: Recall is important in NLP projects where capturing all positive instances is crucial, even if it means accepting some false positives. For instance, in information retrieval tasks like search engines or document retrieval systems, high recall indicates that the model can retrieve most of the relevant documents or information, ensuring that fewer relevant instances are missed.\n",
    "\n",
    "    F1-score: F1-score combines precision and recall into a single metric, making it particularly useful when both avoiding false positives and capturing all positive instances are important. In NLP projects with imbalanced class distributions, where positive or negative instances are sparse, F1-score provides a balanced assessment of the model's performance. It helps strike a balance between precision and recall and is especially relevant when false positives and false negatives have different impacts or costs.\n",
    "\n",
    "These metrics are essential in evaluating the effectiveness and reliability of NLP models, providing insights into their performance, strengths, and weaknesses. They guide the development and improvement of NLP algorithms, helping researchers and practitioners build robust and accurate models for various NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04adf89b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:35:13.240394Z",
     "start_time": "2023-05-30T12:35:06.379022Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 60 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n12 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'roku remove alex jones infowars channel backlash user family killed sandy hook shooting URL'\n\n--------------------------------------------------------------------------------\n48 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'know im crazy late party beat saber spectacular like im kind superhero dj jedi didnt know wanted'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mlogistic_regression, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Perform grid search on the training data\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m     21\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    849\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    850\u001b[0m     )\n\u001b[1;32m--> 852\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 60 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n12 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'roku remove alex jones infowars channel backlash user family killed sandy hook shooting URL'\n\n--------------------------------------------------------------------------------\n48 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'know im crazy late party beat saber spectacular like im kind superhero dj jedi didnt know wanted'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Create an instance of logistic regression\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=logistic_regression, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Perform grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new instance of logistic regression with the best hyperparameters\n",
    "logistic_regression_best = LogisticRegression(**best_params)\n",
    "\n",
    "# Train the logistic regression model with the best hyperparameters\n",
    "logistic_regression_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = logistic_regression_best.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7d63d",
   "metadata": {},
   "source": [
    "GridSearchCV is a class in scikit-learn that provides an automated way to perform hyperparameter tuning using grid search. Grid search is a technique that exhaustively searches the specified hyperparameter values to find the best combination of hyperparameters for a given model.\n",
    "\n",
    "GridSearchCV takes an estimator (e.g., a classifier or a regressor), a parameter grid (a dictionary specifying the hyperparameter values to search), and a cross-validation strategy as input. It then performs an exhaustive search over all possible combinations of hyperparameters specified in the parameter grid. For each combination, it trains and evaluates the model using cross-validation. The best combination of hyperparameters is determined based on a specified scoring metric.\n",
    "\n",
    "The main steps involved in using GridSearchCV are as follows:\n",
    "\n",
    "    Define the parameter grid: Specify the hyperparameters to be tuned and their corresponding values in a dictionary format.\n",
    "\n",
    "    Create an instance of the estimator: Instantiate the estimator (e.g., a classifier or a regressor) with initial hyperparameter values.\n",
    "\n",
    "    Create an instance of GridSearchCV: Pass the estimator and parameter grid as arguments to GridSearchCV, along with the desired cross-validation strategy (e.g., number of folds).\n",
    "\n",
    "    Perform grid search: Call the fit method of GridSearchCV with the training data. This will perform the grid search and find the best combination of hyperparameters.\n",
    "\n",
    "    Get the best hyperparameters: Access the best hyperparameters using the best_params_ attribute of GridSearchCV.\n",
    "\n",
    "    Create a new instance of the estimator with the best hyperparameters: Instantiate the estimator with the best hyperparameters obtained from GridSearchCV.\n",
    "\n",
    "    Train and evaluate the model: Fit the new estimator on the training data and evaluate its performance on the testing data.\n",
    "\n",
    "Using GridSearchCV helps in automating the process of hyperparameter tuning and finding the best hyperparameter values for your model. It saves you from manually trying out different combinations of hyperparameters and provides a systematic way to optimize your model's performance.\n",
    "\n",
    "param_grid is a parameter in GridSearchCV that defines the grid of hyperparameters to search. It is a dictionary where the keys represent the hyperparameter names, and the values are lists or arrays of values to be explored during the grid search.\n",
    "\n",
    "In the context of logistic regression, C, penalty, and solver are commonly used hyperparameters:\n",
    "\n",
    "    C: In logistic regression, C is the inverse of the regularization strength. It controls the trade-off between fitting the training data well and keeping the model simple to avoid overfitting. Smaller values of C result in stronger regularization, while larger values reduce the regularization effect. Typically, C is a positive float value.\n",
    "\n",
    "    penalty: The penalty hyperparameter determines the type of regularization to be applied. Regularization helps prevent overfitting by adding a penalty term to the loss function. Common options for penalty in logistic regression are 'l1' (L1 regularization, also known as Lasso) and 'l2' (L2 regularization, also known as Ridge). L1 regularization tends to produce sparse models with some coefficients set to zero, while L2 regularization encourages small weights for all features.\n",
    "\n",
    "    solver: The solver hyperparameter specifies the algorithm used for optimization during the logistic regression model fitting. Different solvers have different computational characteristics and are suitable for different types of problems. Common choices for solver include 'liblinear', 'lbfgs', 'newton-cg', 'sag', and 'saga'. The 'liblinear' solver is suitable for small-to-medium-sized problems and supports both L1 and L2 penalties.\n",
    "\n",
    "By providing different values for C, penalty, and solver in the param_grid, you can explore different combinations of hyperparameters to find the best configuration that yields the highest model performance. Grid search will systematically evaluate the model using various combinations of these hyperparameter values and identify the best combination based on the specified scoring metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182344c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f3216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:15:50.938345Z",
     "start_time": "2023-05-30T12:15:50.938345Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fix : TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "logistic_regression = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create an instance of LogisticRegression\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "logistic_regression = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Train the logistic regression model using the training data\n",
    "logistic_regression.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1-score: \" + str(f1))\n",
    "\n",
    "#next step\n",
    "# drop one hot encodded columns, only keep the text column. \n",
    "# our clssifier should only predict bussiness class or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6a292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8ab11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5309073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd894a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79b29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
