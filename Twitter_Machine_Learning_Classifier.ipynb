{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "898bb6a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:10:17.796677Z",
     "start_time": "2023-06-10T20:10:16.076734Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from spellchecker import SpellChecker\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61b8e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:10:22.171349Z",
     "start_time": "2023-06-10T20:10:17.799179Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\Owner\\Desktop\\Mangimind Data Science Bootcamp\\Machine Learning Project 3\\tweet_data.csv\\tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed27dddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:10:24.637918Z",
     "start_time": "2023-06-10T20:10:22.172850Z"
    }
   },
   "outputs": [],
   "source": [
    "#Examining general information about the data\n",
    "#df.info()\n",
    "\n",
    "#Filling in the NaN values for photoURL and videoURL \n",
    "df= df. replace(np.nan,'None',regex=True)\n",
    "\n",
    "convert_dict = {'tweetID': str,\n",
    "                'crDate': 'datetime64[ns]',\n",
    "                'rtUsID': str,\n",
    "                'usID': str }\n",
    "df = df.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7187a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:10:25.599726Z",
     "start_time": "2023-06-10T20:10:25.585714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to replace emojis and emoticons with textual descriptions\n",
    "def replace_emojis(text):\n",
    "    # Replace emojis with their textual descriptions\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove the emoji delimiters (colons)\n",
    "    text = re.sub(r':', '', text)\n",
    "    return text\n",
    "\n",
    "# # function for stemming and lemmatization ONLY USE ONE\n",
    "# def stem_and_lemmatize_tokens(tokens):\n",
    "#     stemmer = PorterStemmer()\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return [stemmer.stem(lemmatizer.lemmatize(token)) for token in tokens]\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    tokens = nltk.word_tokenize(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def pos_tagging(text):\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenize the text into words\n",
    "    pos_tags = nltk.pos_tag(tokens)  # Perform POS tagging\n",
    "    return pos_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d8ac71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:10:26.468455Z",
     "start_time": "2023-06-10T20:10:25.601227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         the immediate impulse for an alliance of the e...\n",
       "1         america's economy is flashing some warning sig...\n",
       "2         lyft files for what is expected to be one of t...\n",
       "3         exporters still waiting to get rs 6,000 crore ...\n",
       "4         ride-hailing firm lyft races to leave uber beh...\n",
       "                                ...                        \n",
       "785911              relations are different\\nnot difficult.\n",
       "785912    \"to live a creative life, we must lose our fea...\n",
       "785913      who's your comic crush? https://t.co/h29dhxw3kf\n",
       "785914    after a flight of 195 hours, 18 minutes, 35 se...\n",
       "785915    scarface's action figure tony montana cutting ...\n",
       "Name: text, Length: 785916, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'text' column to strings\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Lowercasing\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd24f49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:10:30.905179Z",
     "start_time": "2023-06-10T20:10:26.471458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         the immediate impulse for an alliance of the e...\n",
       "1         americas economy is flashing some warning sign...\n",
       "2         lyft files for what is expected to be one of t...\n",
       "3         exporters still waiting to get rs 6000 crore w...\n",
       "4         ridehailing firm lyft races to leave uber behi...\n",
       "                                ...                        \n",
       "785911               relations are different\\nnot difficult\n",
       "785912    to live a creative life we must lose our fear ...\n",
       "785913             whos your comic crush httpstcoh29dhxw3kf\n",
       "785914    after a flight of 195 hours 18 minutes 35 seco...\n",
       "785915    scarfaces action figure tony montana cutting o...\n",
       "Name: text, Length: 785916, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a000e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:10:51.289287Z",
     "start_time": "2023-06-10T20:10:30.906680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         the immediate impulse for an alliance of the e...\n",
       "1         americas economy is flashing some warning sign...\n",
       "2         lyft files for what is expected to be one of t...\n",
       "3         exporters still waiting to get rs 6000 crore w...\n",
       "4         ridehailing firm lyft races to leave uber behi...\n",
       "                                ...                        \n",
       "785911               relations are different\\nnot difficult\n",
       "785912    to live a creative life we must lose our fear ...\n",
       "785913             whos your comic crush httpstcoh29dhxw3kf\n",
       "785914    after a flight of 195 hours 18 minutes 35 seco...\n",
       "785915    scarfaces action figure tony montana cutting o...\n",
       "Name: text, Length: 785916, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace emojis and emoticons with textual descriptions\n",
    "df['text'] = df['text'].apply(replace_emojis)\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af2b5ea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:12:26.818462Z",
     "start_time": "2023-06-10T20:10:51.290788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "df['text'] = df['text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ae5f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:12:26.833976Z",
     "start_time": "2023-06-10T20:12:26.819963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [the, immediate, impulse, for, an, alliance, o...\n",
       "1         [americas, economy, is, flashing, some, warnin...\n",
       "2         [lyft, files, for, what, is, expected, to, be,...\n",
       "3         [exporters, still, waiting, to, get, rs, 6000,...\n",
       "4         [ridehailing, firm, lyft, races, to, leave, ub...\n",
       "                                ...                        \n",
       "785911          [relations, are, different, not, difficult]\n",
       "785912    [to, live, a, creative, life, we, must, lose, ...\n",
       "785913       [whos, your, comic, crush, httpstcoh29dhxw3kf]\n",
       "785914    [after, a, flight, of, 195, hours, 18, minutes...\n",
       "785915    [scarfaces, action, figure, tony, montana, cut...\n",
       "Name: text, Length: 785916, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d80724a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.362329Z",
     "start_time": "2023-06-10T20:12:26.834976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [the, immediate, impulse, for, an, alliance, o...\n",
       "1         [americas, economy, is, flashing, some, warnin...\n",
       "2         [lyft, files, for, what, is, expected, to, be,...\n",
       "3         [exporters, still, waiting, to, get, rs, 6000,...\n",
       "4         [ridehailing, firm, lyft, races, to, leave, ub...\n",
       "                                ...                        \n",
       "785911          [relations, are, different, not, difficult]\n",
       "785912    [to, live, a, creative, life, we, must, lose, ...\n",
       "785913                         [whos, your, comic, crush, ]\n",
       "785914    [after, a, flight, of, 195, hours, 18, minutes...\n",
       "785915    [scarfaces, action, figure, tony, montana, cut...\n",
       "Name: text, Length: 785916, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling URLs\n",
    "df['text'] = df['text'].apply(lambda x: [re.sub(r'http\\S+|www\\S+', '', word) for word in x])\n",
    "\n",
    "# Handling mentions\n",
    "df['text'] = df['text'].apply(lambda x: [re.sub(r'@[\\w_]+', '', word) for word in x])\n",
    "\n",
    "# Handling hashtags\n",
    "df['text'] = df['text'].apply(lambda x: [re.sub(r'#\\w+', '', word) for word in x])\n",
    "\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bb56a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:20:39.852755Z",
     "start_time": "2023-06-10T20:20:39.836742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                           [the, immediate, impulse, for, an, alliance, of, the, eus, northern, states, is, brexit, , ]\n",
       "1                                                                                                                                                                         [americas, economy, is, flashing, some, warning, signs, but, for, now, the, labor, market, appears, to, be, going, strong, , ]\n",
       "2                                                                                                                                                                                                                [lyft, files, for, what, is, expected, to, be, one, of, the, hottest, ipos, in, 2019, ]\n",
       "3         [exporters, still, waiting, to, get, rs, 6000, crore, worth, of, input, tax, credit, refunds, many, being, denied, tax, refunds, by, state, governments, such, as, andhra, pradesh, uttar, pradesh, bihar, and, chhattisgarh, who, say, they, are, cash, starved, subhayan_ism, gst_council, ]\n",
       "4                                                                                                                                                                                                                          [ridehailing, firm, lyft, races, to, leave, uber, behind, in, ipo, chase, , ]\n",
       "                                                                                                                                                       ...                                                                                                                                              \n",
       "785911                                                                                                                                                                                                                                                       [relations, are, different, not, difficult]\n",
       "785912                                                                                                                                                                                                                      [to, live, a, creative, life, we, must, lose, our, fear, of, being, wrong, ]\n",
       "785913                                                                                                                                                                                                                                                                      [whos, your, comic, crush, ]\n",
       "785914                                   [after, a, flight, of, 195, hours, 18, minutes, 35, seconds, the, apollo11, crew, splashed, down, in, the, north, pacific, ocean, 900, miles, southwest, of, hawaii, heres, a, photo, of, their, recovery, as, we, celebrate, the, apollo50th, anniversary, , ]\n",
       "785915                                                                                                                                                                       [scarfaces, action, figure, tony, montana, cutting, open, a, pack, of, flour, on, a, kitchen, table, by, artist, vse, ok, ]\n",
       "Name: text, Length: 785916, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56e93a4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:20:56.017322Z",
     "start_time": "2023-06-10T20:20:41.390046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove additional characters and punctuation marks\n",
    "df['text'] = df['text'].apply(lambda x: [re.sub(r'[^A-Za-z]', '', token) \n",
    "                                         for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad0f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T15:53:32.635445Z",
     "start_time": "2023-06-10T15:53:32.625937Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41b074",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-10T20:20:44.201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████████████████████████████████████████| 786/786 [00:00<00:00, 78538.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Create an instance of the enchant spell checker\n",
    "spell_checker = enchant.Dict(\"en_US\")\n",
    "\n",
    "# Function to correct the spelling of a word using the spell checker\n",
    "def correct_spelling(word):\n",
    "    if not word:\n",
    "        return word  # Skip empty strings\n",
    "    if not spell_checker.check(word):\n",
    "        suggestions = spell_checker.suggest(word)\n",
    "        if suggestions:\n",
    "            return suggestions[0]  # Choose the first suggested correction\n",
    "        else:\n",
    "            return \"UNKNOWN\"  # Word not found in dictionary, mark as unknown\n",
    "    return word\n",
    "\n",
    "# Function to apply spell checking to a chunk of data\n",
    "def process_chunk(chunk):\n",
    "    return [correct_spelling(word) for word in chunk]\n",
    "\n",
    "# Get the number of available CPU cores\n",
    "num_processes = cpu_count()\n",
    "\n",
    "# Split the data into chunks\n",
    "chunks = [df['text'][i:i+1000] for i in range(0, len(df['text']), 1000)]\n",
    "\n",
    "# Create a pool of worker processes\n",
    "pool = Pool(processes=num_processes)\n",
    "\n",
    "# Apply spell checking to each chunk of data in parallel\n",
    "results = []\n",
    "with tqdm(total=len(chunks), desc=\"Processing\") as pbar:\n",
    "    for chunk in chunks:\n",
    "        results.append(pool.apply_async(process_chunk, (chunk,)))\n",
    "        pbar.update(1)\n",
    "\n",
    "# Wait for all processes to complete\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Get the results from the async processes\n",
    "results = [res.get() for res in results]\n",
    "\n",
    "# Combine the results into the final output\n",
    "corrected_data = [word for chunk in results for word in chunk]\n",
    "\n",
    "# Update the 'text' column with the corrected data\n",
    "df['text'] = corrected_data\n",
    "\n",
    "# Print the corrected 'text' column\n",
    "print(df['text'].head())\n",
    "print(\"Spell checking completed!\")\n",
    "\n",
    "# Force stop the execution\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea0b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.405866Z",
     "start_time": "2023-06-10T20:13:08.405866Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663cb641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.406867Z",
     "start_time": "2023-06-10T20:13:08.406867Z"
    }
   },
   "outputs": [],
   "source": [
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# spell = SpellChecker()\n",
    "\n",
    "# def correct_spelling(word_list):\n",
    "#     corrected_words = []\n",
    "    \n",
    "#     for word in word_list:\n",
    "#         corrected_word = spell.correction(word)\n",
    "        \n",
    "#         if corrected_word != word:\n",
    "#             corrected_words.append(corrected_word)\n",
    "#         else:\n",
    "#             corrected_words.append(word)\n",
    "    \n",
    "#     return corrected_words\n",
    "\n",
    "# df['text'] = df['text'].apply(correct_spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b07799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.407868Z",
     "start_time": "2023-06-10T20:13:08.407868Z"
    }
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "import pandas as pd\n",
    "\n",
    "# Create an instance of the enchant spell checker\n",
    "spell_checker = enchant.Dict(\"en_US\")\n",
    "\n",
    "# Function to correct the spelling of a word using the spell checker\n",
    "def correct_spelling(word):\n",
    "    if not spell_checker.check(word):\n",
    "        suggestions = spell_checker.suggest(word)\n",
    "        if suggestions:\n",
    "            return suggestions[0]  # Choose the first suggested correction\n",
    "    return word\n",
    "\n",
    "# Apply spell checking to the 'text' column\n",
    "df['text'] = df['text'].apply(lambda x: [correct_spelling(word) for word in x])\n",
    "\n",
    "# Print the corrected 'text' column\n",
    "print(df['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cef8bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.409369Z",
     "start_time": "2023-06-10T20:13:08.409369Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token]\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "\n",
    "# Remove additional characters and punctuation marks\n",
    "df['text'] = df['text'].apply(lambda x: [re.sub(r'[^A-Za-z]', '', token) \n",
    "                                         for token in x])\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: lemmatize_tokens(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca31f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.409869Z",
     "start_time": "2023-06-10T20:13:08.409869Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# def get_wordnet_pos(tag):\n",
    "#     if tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     elif tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     else:\n",
    "#         return wordnet.NOUN  # default to noun if the POS tag is unknown\n",
    "\n",
    "# def lemmatize_tokens(tokens):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tagged_tokens = pos_tag(tokens)\n",
    "#     return [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens if token]\n",
    "\n",
    "# df['text'] = df['text'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "\n",
    "# # Remove additional characters and punctuation marks\n",
    "# df['text'] = df['text'].apply(lambda x: [re.sub(r'[^A-Za-z]', '', token) for token in x])\n",
    "\n",
    "# df['text'] = df['text'].apply(lambda x: lemmatize_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf56ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.410870Z",
     "start_time": "2023-06-10T20:13:08.410870Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42ce17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.411871Z",
     "start_time": "2023-06-10T20:13:08.411871Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302261b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.412872Z",
     "start_time": "2023-06-10T20:13:08.412872Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Convert 'text' column to strings\n",
    "# df['text'] = df['text'].astype(str)\n",
    "\n",
    "# # Lowercasing\n",
    "# df['text'] = df['text'].str.lower()\n",
    "\n",
    "# # Removing punctuation\n",
    "# df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# # Replace emojis and emoticons with textual descriptions\n",
    "# df['text'] = df['text'].apply(replace_emojis)\n",
    "\n",
    "# # Handling URLs\n",
    "# df['text'] = df['text'].apply(lambda x: [re.sub(r'http\\S+|www\\S+', '', word) for word in x])\n",
    "\n",
    "# # Handling mentions\n",
    "# df['text'] = df['text'].apply(lambda x: [re.sub(r'@[\\w_]+', '', word) for word in x])\n",
    "\n",
    "# # Handling hashtags\n",
    "# df['text'] = df['text'].apply(lambda x: [re.sub(r'#\\w+', '', word) for word in x])\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "#df['text'] = df['text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# # Apply POS tagging to the 'text' column\n",
    "# df['text'] = df['text'].apply(lambda x: ' '.join(x))  # Convert the list of tokens to a string\n",
    "# df['text'] = df['text'].apply(pos_tagging)\n",
    "\n",
    "# Apply stemming or lemmatization\n",
    "#df['text'] = df['text'].apply(lambda x: stem_tokens(x)) # or lemmatize_tokens(X)\n",
    "# df['text'] = df['text'].apply(lambda x: lemmatize_tokens(x)) # or stem_tokens(X)\n",
    "# df['text'] = df['text'].apply(lambda x: lemmatize_tokens(list(x)))\n",
    "\n",
    "# # Removing stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "\n",
    "# Initialize the spell checker\n",
    "# spell = SpellChecker()\n",
    "\n",
    "# # Function to correct misspelled words in a text\n",
    "# def correct_spelling(text):\n",
    "#     # Tokenize the text into words\n",
    "#     words = text.split()\n",
    "    \n",
    "#     # Find and correct misspelled words\n",
    "#     corrected_words = []\n",
    "#     for word in words:\n",
    "#         # Check if the word is misspelled\n",
    "#         if not spell.correction(word) == word:\n",
    "#             # Correct the misspelled word\n",
    "#             corrected_word = spell.correction(word)\n",
    "#             corrected_words.append(corrected_word)\n",
    "#         else:\n",
    "#             corrected_words.append(word)\n",
    "    \n",
    "#     # Join the corrected words back into a sentence\n",
    "#     corrected_text = ' '.join(corrected_words)\n",
    "    \n",
    "#     return corrected_text\n",
    "\n",
    "# # Apply spell checking to the 'text' column\n",
    "# df['text'] = df['text'].apply(correct_spelling)\n",
    "# Print the preprocessed text column\n",
    "# print(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702631f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.413873Z",
     "start_time": "2023-06-10T20:13:08.413873Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71a3be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.414873Z",
     "start_time": "2023-06-10T20:13:08.414873Z"
    }
   },
   "outputs": [],
   "source": [
    "# def lemmatize_tokens(tokens):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# df['text'] = df['text'].apply(lambda x: lemmatize_tokens(x))\n",
    "\n",
    "# def word_lemmatize(string):\n",
    "#   output = [WordNetLemmatizer().lemmatize(w) for w in string]\n",
    "#   return output\n",
    "\n",
    "# df['text']= df['text'].apply(word_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c96eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.415875Z",
     "start_time": "2023-06-10T20:13:08.415875Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373abcc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.416876Z",
     "start_time": "2023-06-10T20:13:08.416876Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5796d",
   "metadata": {},
   "source": [
    "What is baseline classifier in machine learning?\n",
    "A baseline model is essentially a simple model that acts as a reference in a machine learning project. Its main function is to contextualize the results of trained models. Baseline models usually lack complexity and may have little predictive power. Regardless, their inclusion is a necessity for many reasons.\n",
    "\n",
    "We need a baseline performance and for NLP we just tokenize without doing much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb822a",
   "metadata": {},
   "source": [
    "Removing punctuation\n",
    "This line uses the apply() method along with a lambda function to remove punctuation from each text in the 'text' column.\n",
    "The regular expression r'[^\\w\\s]' matches any non-alphanumeric and non-whitespace characters, and re.sub() replaces them with an empty string.\n",
    "\n",
    "\n",
    "Tokenization\n",
    "This line applies the word_tokenize() function from NLTK to tokenize each text in the 'text' column. \n",
    "Tokenization splits the text into individual words or tokens.\n",
    "\n",
    "\n",
    "Removing stopwords\n",
    "These lines remove stopwords from each text in the 'text' column. First, a set of stopwords for the English language is created using stopwords.words('english'). \n",
    "Then, a lambda function is applied using the apply() method, which iterates over each token in the text and keeps only the words that are not in the set of stopwords.\n",
    "\n",
    "Stemming\n",
    "These lines perform stemming on each token in the 'text' column using the Porter stemming algorithm.\n",
    "First, a PorterStemmer object is created. Then, a lambda function is applied using the apply() method, \n",
    "which iterates over each token in the text and applies the stemming algorithm to reduce each word to its base or root form.\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or root form, which is called a lemma. The lemma represents the canonical or dictionary form of a word, from which all inflected forms of the word can be generated.\n",
    "In English, lemmatization typically involves removing inflections such as plurals, verb conjugations, and adverb or adjective endings to produce the base form of the word. For example, the lemma of the word \"running\" is \"run,\" and the lemma of the word \"better\" is \"good.\"\n",
    "\n",
    "Lemmatization is commonly used in natural language processing (NLP) and text analysis tasks to normalize words and reduce vocabulary size. By reducing words to their lemmas, different forms of the same word are treated as a single token, which can improve the accuracy and efficiency of various NLP algorithms and models.\n",
    "\n",
    "Question?\n",
    "\n",
    "- Treat as separate features: If emojis and emoticons carry sentiment or additional meaning that is important for your task, you can treat them as separate features and preserve them in the text. You can encode them uniquely or represent them using special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b67e65",
   "metadata": {},
   "source": [
    "The preprocessing steps (replacing emojis and emoticons, lowercasing text, removing punctuation, tokenization, removing stopwords, applying stemming and lemmatization, handling URLs, mentions, and hashtags) are commonly used in text preprocessing for machine learning classifiers. Here's why each step is important:\n",
    "\n",
    "Replacing emojis and emoticons: Emojis and emoticons are graphical representations of emotions, and they don't carry much meaning in textual analysis. By replacing them with textual descriptions, you can convert them into meaningful words or phrases that can contribute to the understanding of the text.\n",
    "\n",
    "Lowercasing text: Lowercasing the text helps in normalization by treating different cases of the same word as identical. For example, \"apple\" and \"Apple\" will be treated as the same word.\n",
    "\n",
    "Removing punctuation: Punctuation marks don't usually contribute much to the overall meaning of the text. Removing them simplifies the text and reduces noise in the data.\n",
    "\n",
    "Tokenization: Tokenization is the process of breaking the text into individual words or tokens. It helps in further analysis and allows the model to understand the context and meaning of each word.\n",
    "\n",
    "Removing stopwords: Stopwords are common words that occur frequently in a language (e.g., \"the\", \"is\", \"and\"). These words generally don't add much value to the analysis as they are commonly used and don't carry significant meaning. Removing them helps reduce the dimensionality of the data and focuses on more important words.\n",
    "\n",
    "Applying stemming and lemmatization: Stemming and lemmatization are techniques used to reduce words to their base or root form. This helps in consolidating words with the same meaning and reducing the vocabulary size. For example, \"running,\" \"runs,\" and \"ran\" can all be stemmed to \"run.\"\n",
    "\n",
    "Handling URLs, mentions, and hashtags: URLs, mentions of usernames (e.g., \"@username\"), and hashtags (e.g., \"#example\") are specific patterns in text that can be treated differently. Handling them involves replacing URLs with a generic token, replacing mentions with a common username, and extracting the meaningful word or phrase from a hashtag.\n",
    "\n",
    "By applying these preprocessing steps, you can transform the raw text into a format that is more suitable for machine learning classifiers. It helps in reducing noise, capturing important information, and improving the performance of the classifier by focusing on relevant features.Note ONLY USE ONE\n",
    "\n",
    "Feature vectors are created for machine learning classifiers to represent the input data in a numerical format that can be processed by the algorithms. Here are the key reasons why feature vectors are important for machine learning classifiers:\n",
    "\n",
    "- Numerical Representation: Machine learning classifiers operate on numerical data. By representing the input data as feature vectors, we can leverage mathematical operations and statistical techniques that are essential for training and making predictions with classifiers.\n",
    "\n",
    "- Information Extraction: Feature vectors allow us to extract relevant information from the input data. By carefully selecting and designing the features, we can capture the key characteristics or patterns that are indicative of the class labels we want the classifier to learn and predict. These features can include numerical values, text attributes, categorical variables, or any other relevant data representations.\n",
    "\n",
    "- Model Training and Prediction: Machine learning classifiers learn patterns and relationships between the feature vectors and the corresponding class labels. During the training phase, the classifier analyzes the feature vectors to create a model that can generalize to unseen data. The feature vectors serve as the input to train the classifier, allowing it to learn the underlying patterns and make accurate predictions on new instances.\n",
    "\n",
    "- Dimensionality Reduction: Feature vectors can help in reducing the dimensionality of the input data. High-dimensional data can be computationally expensive and may suffer from the curse of dimensionality. Feature extraction or selection techniques can be applied to derive a lower-dimensional representation while preserving important information, improving the efficiency and performance of the classifier.\n",
    "\n",
    "- Standardization and Normalization: Feature vectors can be standardized or normalized to ensure consistent scales across different features. This is particularly important when features have varying units, ranges, or distributions. Standardization allows the classifier to treat all features equally, preventing certain features from dominating the learning process.\n",
    "\n",
    "- By creating appropriate feature vectors, we enable machine learning classifiers to learn from the data and make accurate predictions or classifications on new instances. The choice and design of feature vectors play a critical role in the performance and effectiveness of machine learning classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685bc83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.417876Z",
     "start_time": "2023-06-10T20:13:08.417876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(df['topicName'])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "# Remove the original 'topicName' column\n",
    "df_encoded.drop('topicName', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff25044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T00:33:32.696809Z",
     "start_time": "2023-05-30T00:33:32.682793Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "One-hot encoding is commonly used for machine learning classifiers, especially when dealing with categorical variables. Many machine learning algorithms, such as logistic regression, support vector machines, and neural networks, require numerical inputs. One-hot encoding is a technique used to represent categorical variables as binary features, which can be understood and processed by these algorithms.\n",
    "\n",
    "In one-hot encoding, each category of a categorical variable is represented by a binary feature column. For a variable with n categories, n binary feature columns are created, where each column indicates whether the corresponding category is present or not. The value 1 is assigned to the column representing the category, while all other columns are filled with 0s.\n",
    "\n",
    "By performing one-hot encoding, categorical variables can be effectively incorporated into the input data, allowing the classifier to learn patterns and make predictions based on these variables. It enables the classifier to understand and utilize the information conveyed by the different categories of the variable.\n",
    "\n",
    "However, it's important to note that one-hot encoding can increase the dimensionality of the feature space, which may impact the performance of the classifier, especially if the number of categories is large. In such cases, feature selection or dimensionality reduction techniques may be applied to mitigate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485656a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.418877Z",
     "start_time": "2023-06-10T20:13:08.418877Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "df_encoded['edInput'].value_counts()\n",
    "# -1 unlabled data\n",
    "# 1 confirmed by the editor that they are business calss\n",
    "# 2 they confirmed they are misclassified. Editor does not agree\n",
    "# 3 editor is confused\n",
    "# 4 they are business but they were posted previously from another channel. If 4  it will noty be posted ot the user.\n",
    "# eliminate three and four. Might want to included by relabling 4 as 1.\n",
    "# stratification means your test splites has the same representation of all the classes. If your original has 30% of label 1 and 60 of label 2 your "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406339e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.419377Z",
     "start_time": "2023-06-10T20:13:08.419377Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee740f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.420378Z",
     "start_time": "2023-06-10T20:13:08.420378Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df = df_encoded[df_encoded['edInput'].isin([1,2])]\n",
    "filtered_df['edInput'] = filtered_df['edInput'].replace(2,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a512b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.421879Z",
     "start_time": "2023-06-10T20:13:08.421879Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df['edInput'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3854bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.422880Z",
     "start_time": "2023-06-10T20:13:08.422880Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = filtered_df['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11990165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T00:33:32.704316Z",
     "start_time": "2023-05-30T00:33:32.704316Z"
    }
   },
   "source": [
    " There are several machine learning models that are commonly used for text classification. The choice of model depends on various factors such as the size of the dataset, the complexity of the classification task, and the specific requirements of your project. Here are some popular models for text classification:\n",
    "\n",
    "    Naive Bayes: Naive Bayes is a simple and efficient probabilistic classifier. It works well with text data and is often used as a baseline model for text classification tasks.\n",
    "\n",
    "    Support Vector Machines (SVM): SVM is a powerful and versatile model for text classification. It can handle high-dimensional data and is known for its ability to find complex decision boundaries.\n",
    "\n",
    "    Random Forest: Random Forest is an ensemble model that combines multiple decision trees to make predictions. It can handle text data and is robust against overfitting.\n",
    "\n",
    "    Logistic Regression: Logistic Regression is a simple and interpretable model that works well for binary text classification tasks. It uses a logistic function to model the probability of the input belonging to a certain class.\n",
    "\n",
    "    Neural Networks: Neural Networks, especially variants like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), have shown promising results in text classification tasks. They can capture complex patterns and dependencies in the text data.\n",
    "\n",
    "The choice of model ultimately depends on the specific requirements of your project and the characteristics of your data. It's often a good idea to experiment with multiple models and compare their performance using appropriate evaluation metrics to find the best model for your text classification task.\n",
    "\n",
    "Yes, applying vectorization is typically done on the training dataset in NLP projects. Vectorization is the process of converting text data into numerical representations that can be used as input to machine learning models. This is necessary because most machine learning algorithms require numerical input.\n",
    "\n",
    "In the context of NLP, vectorization techniques such as bag-of-words (BoW), term frequency-inverse document frequency (TF-IDF), or word embeddings like Word2Vec or GloVe are commonly used. These techniques transform text data into numerical feature vectors that capture the semantic or syntactic information present in the text.\n",
    "\n",
    "When applying vectorization, it is important to fit the vectorizer (e.g., CountVectorizer or TfidfVectorizer) on the training data and then transform both the training and testing data using the fitted vectorizer. This ensures that the same vocabulary and feature representation are used consistently across the training and testing sets.\n",
    "\n",
    "By applying both CountVectorizer and TfidfVectorizer, you generate different representations of the text data. CountVectorizer represents the frequency of each word, while TfidfVectorizer represents the importance of each word in the document and the entire corpus. These different representations can be used as inputs for various NLP tasks, such as classification, clustering, or information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9d3ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.423881Z",
     "start_time": "2023-06-10T20:13:08.423881Z"
    }
   },
   "outputs": [],
   "source": [
    "X = filtered_df['text'].astype(str)\n",
    "y = filtered_df['edInput']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply fit_transform to the training data\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply transform to the testing data\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply fit_transform to the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply transform to the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Create an instance of LogisticRegression with increased max_iter\n",
    "logistic_regression = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Train the logistic regression model using the training data\n",
    "logistic_regression.fit(X_train_count, y_train)\n",
    "\n",
    "# Make predictions on the testing data using CountVectorizer features\n",
    "y_pred_count = logistic_regression.predict(X_test_count)\n",
    "\n",
    "# Train the logistic regression model using the training data\n",
    "logistic_regression.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the testing data using TF-IDF features\n",
    "y_pred_tfidf = logistic_regression.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the models\n",
    "accuracy_count = accuracy_score(y_test, y_pred_count)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "precision_count = precision_score(y_test, y_pred_count)\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "\n",
    "recall_count = recall_score(y_test, y_pred_count)\n",
    "recall_tfidf = recall_score(y_test, y_pred_tfidf)\n",
    "\n",
    "f1_count = f1_score(y_test, y_pred_count)\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
    "\n",
    "print(\"CountVectorizer Accuracy: \", accuracy_count)\n",
    "print(\"TF-IDF Accuracy: \", accuracy_tfidf)\n",
    "print(\"CountVectorizer Precision: \", precision_count)\n",
    "print(\"TF-IDF Precision: \", precision_tfidf)\n",
    "print(\"CountVectorizer Recall: \", recall_count)\n",
    "print(\"TF-IDF Recall: \", recall_tfidf)\n",
    "print(\"CountVectorizer F1-score: \", f1_count)\n",
    "print(\"TF-IDF F1-score: \", f1_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69587a84",
   "metadata": {},
   "source": [
    "The conversion of elements in X_train and X_test to strings is necessary because the fit_transform and transform methods of CountVectorizer expect input data in the form of iterable over strings or bytes-like objects.\n",
    "\n",
    "In the code you provided, the original X_train and X_test are lists, and it's possible that some elements in these lists are not strings. By converting all elements to strings using str(x), we ensure that all elements in X_train and X_test are valid inputs for CountVectorizer.\n",
    "\n",
    "If your X_train and X_test already consist of strings, you can omit the conversion step and directly use them in the fit_transform and transform methods of CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab0009",
   "metadata": {},
   "source": [
    "In NLP (Natural Language Processing) projects, these evaluation metrics hold particular significance as they help assess the performance of models that deal with text data. Here's their significance in the context of NLP projects:\n",
    "\n",
    "    Accuracy: In NLP tasks, such as sentiment analysis, text classification, or spam detection, accuracy indicates the overall correctness of the model's predictions. It helps evaluate how well the model can correctly classify or predict the intended labels or categories for the given text data. High accuracy signifies that the model is making correct predictions, which is crucial for reliable results in NLP applications.\n",
    "\n",
    "   false positive - Precision: Precision is valuable in NLP projects where the focus is on avoiding false positive predictions. For example, in text classification for medical diagnosis or identifying hate speech, precision helps evaluate the model's ability to correctly classify positive instances while minimizing false positives. High precision implies that the model is better at avoiding false positives, ensuring that the predicted positive instances are indeed relevant or meaningful.\n",
    "\n",
    "    false negative - Recall: Recall is important in NLP projects where capturing all positive instances is crucial, even if it means accepting some false positives. For instance, in information retrieval tasks like search engines or document retrieval systems, high recall indicates that the model can retrieve most of the relevant documents or information, ensuring that fewer relevant instances are missed.\n",
    "\n",
    "    F1-score: F1-score combines precision and recall into a single metric, making it particularly useful when both avoiding false positives and capturing all positive instances are important. In NLP projects with imbalanced class distributions, where positive or negative instances are sparse, F1-score provides a balanced assessment of the model's performance. It helps strike a balance between precision and recall and is especially relevant when false positives and false negatives have different impacts or costs.\n",
    "\n",
    "These metrics are essential in evaluating the effectiveness and reliability of NLP models, providing insights into their performance, strengths, and weaknesses. They guide the development and improvement of NLP algorithms, helping researchers and practitioners build robust and accurate models for various NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04adf89b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.424882Z",
     "start_time": "2023-06-10T20:13:08.424882Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# X = text_data\n",
    "# y = filtered_df['edInput']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(filtered_df['text'].astype(str), y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Create an instance of the TF-IDF vectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Apply the vectorizer to the training data\n",
    "# X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Apply the vectorizer to the testing data\n",
    "# X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# # Scale the TF-IDF data\n",
    "# scaler = MaxAbsScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train_tfidf)\n",
    "# X_test_scaled = scaler.transform(X_test_tfidf)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Create an instance of logistic regression\n",
    "logistic_regression = LogisticRegression(max_iter=4000)\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=logistic_regression, param_grid=param_grid, cv=5, n_jobs = -1)\n",
    "\n",
    "# Perform grid search on the training data\n",
    "grid_search.fit(X_train_count, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new instance of logistic regression with the best hyperparameters\n",
    "logistic_regression_best = LogisticRegression(**best_params)\n",
    "\n",
    "# Train the logistic regression model with the best hyperparameters\n",
    "logistic_regression_best.fit(X_train_count, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = logistic_regression_best.predict(X_train_count)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "precision = precision_score(y_train, y_pred)\n",
    "recall = recall_score(y_train, y_pred)\n",
    "f1 = f1_score(y_train, y_pred)\n",
    "9\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10833fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.425883Z",
     "start_time": "2023-06-10T20:13:08.425883Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train the SVM classifier using the training data\n",
    "svm_classifier.fit(X_train_count, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = svm_classifier.predict(X_train_count)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7d63d",
   "metadata": {},
   "source": [
    "Want to see more classifiers and more nlp pipline process. in count vectorizer, you call the default parameters, there are a few paraemeters you can play with \n",
    "GridSearchCV is a class in scikit-learn that provides an automated way to perform hyperparameter tuning using grid search. Grid search is a technique that exhaustively searches the specified hyperparameter values to find the best combination of hyperparameters for a given model.\n",
    "\n",
    "GridSearchCV takes an estimator (e.g., a classifier or a regressor), a parameter grid (a dictionary specifying the hyperparameter values to search), and a cross-validation strategy as input. It then performs an exhaustive search over all possible combinations of hyperparameters specified in the parameter grid. For each combination, it trains and evaluates the model using cross-validation. The best combination of hyperparameters is determined based on a specified scoring metric.\n",
    "\n",
    "The main steps involved in using GridSearchCV are as follows:\n",
    "\n",
    "    Define the parameter grid: Specify the hyperparameters to be tuned and their corresponding values in a dictionary format.\n",
    "\n",
    "    Create an instance of the estimator: Instantiate the estimator (e.g., a classifier or a regressor) with initial hyperparameter values.\n",
    "\n",
    "    Create an instance of GridSearchCV: Pass the estimator and parameter grid as arguments to GridSearchCV, along with the desired cross-validation strategy (e.g., number of folds).\n",
    "\n",
    "    Perform grid search: Call the fit method of GridSearchCV with the training data. This will perform the grid search and find the best combination of hyperparameters.\n",
    "\n",
    "    Get the best hyperparameters: Access the best hyperparameters using the best_params_ attribute of GridSearchCV.\n",
    "\n",
    "    Create a new instance of the estimator with the best hyperparameters: Instantiate the estimator with the best hyperparameters obtained from GridSearchCV.\n",
    "\n",
    "    Train and evaluate the model: Fit the new estimator on the training data and evaluate its performance on the testing data.\n",
    "\n",
    "Using GridSearchCV helps in automating the process of hyperparameter tuning and finding the best hyperparameter values for your model. It saves you from manually trying out different combinations of hyperparameters and provides a systematic way to optimize your model's performance.\n",
    "\n",
    "param_grid is a parameter in GridSearchCV that defines the grid of hyperparameters to search. It is a dictionary where the keys represent the hyperparameter names, and the values are lists or arrays of values to be explored during the grid search.\n",
    "\n",
    "In the context of logistic regression, C, penalty, and solver are commonly used hyperparameters:\n",
    "\n",
    "    C: In logistic regression, C is the inverse of the regularization strength. It controls the trade-off between fitting the training data well and keeping the model simple to avoid overfitting. Smaller values of C result in stronger regularization, while larger values reduce the regularization effect. Typically, C is a positive float value.\n",
    "\n",
    "    penalty: The penalty hyperparameter determines the type of regularization to be applied. Regularization helps prevent overfitting by adding a penalty term to the loss function. Common options for penalty in logistic regression are 'l1' (L1 regularization, also known as Lasso) and 'l2' (L2 regularization, also known as Ridge). L1 regularization tends to produce sparse models with some coefficients set to zero, while L2 regularization encourages small weights for all features.\n",
    "\n",
    "    solver: The solver hyperparameter specifies the algorithm used for optimization during the logistic regression model fitting. Different solvers have different computational characteristics and are suitable for different types of problems. Common choices for solver include 'liblinear', 'lbfgs', 'newton-cg', 'sag', and 'saga'. The 'liblinear' solver is suitable for small-to-medium-sized problems and supports both L1 and L2 penalties.\n",
    "\n",
    "By providing different values for C, penalty, and solver in the param_grid, you can explore different combinations of hyperparameters to find the best configuration that yields the highest model performance. Grid search will systematically evaluate the model using various combinations of these hyperparameter values and identify the best combination based on the specified scoring metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182344c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.426884Z",
     "start_time": "2023-06-10T20:13:08.426884Z"
    }
   },
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f3216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.427884Z",
     "start_time": "2023-06-10T20:13:08.427884Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Fix : TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "# logistic_regression = LogisticRegression(max_iter=1000)\n",
    "# logistic_regression = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# # Create an instance of LogisticRegression\n",
    "# logistic_regression = LogisticRegression()\n",
    "\n",
    "# logistic_regression = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# # Train the logistic regression model using the training data\n",
    "# logistic_regression.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Accuracy: \" + str(accuracy))\n",
    "# print(\"Precision: \" + str(precision))\n",
    "# print(\"Recall: \" + str(recall))\n",
    "# print(\"F1-score: \" + str(f1))\n",
    "\n",
    "# #next step\n",
    "# # drop one hot encodded columns, only keep the text column. \n",
    "# # our clssifier should only predict bussiness class or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e91da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.428885Z",
     "start_time": "2023-06-10T20:13:08.428885Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(bow_features, df_encoded['Business'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # The purpose of this code is to extract different types of text features, such as Bag-of-Words, TF-IDF representations, \n",
    "# # and word embeddings, from the preprocessed text data. \n",
    "# # These features can be used as inputs for various machine learning or natural language processing tasks. \n",
    "# # The code ensures that feature extraction is performed only when there are valid documents available for analysis.\n",
    "# # Convert the elements in the 'text' column to strings\n",
    "# text_data = [' '.join(text) for text in df['text']]\n",
    "\n",
    "# # Create an instance of CountVectorizer\n",
    "# bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the text data\n",
    "# bow_features = bow_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# # # Print bow_features\n",
    "# # print(bow_features)\n",
    "\n",
    "# # Create an instance of TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the text data\n",
    "# tfidf_features = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# # Print tfidf_features\n",
    "# print(tfidf_features)\n",
    "\n",
    "# # # Word embeddings\n",
    "# # word2vec_model = gensim.models.Word2Vec(sentences=df['text'], vector_size=100, min_count=1)\n",
    "\n",
    "# # for text in df['text']:\n",
    "# #      word_embeddings = word2vec_model.wv[text]\n",
    "# #      print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6a292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee733d90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.429887Z",
     "start_time": "2023-06-10T20:13:08.429887Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Convert the elements in the 'text' column to strings\n",
    "# text_data = [' '.join(text) for text in filtered_df['text']]\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(text_data, filtered_df['edInput'], test_size=0.2, random_state=42, stratify= filtered_df['edInput']  )\n",
    "\n",
    "# # Create an instance of CountVectorizer\n",
    "# count_vectorizer = CountVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the training data\n",
    "# X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Apply transform to the testing data\n",
    "# X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# # Create an instance of TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the training data\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Apply transform to the testing data\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5527ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T20:13:08.430886Z",
     "start_time": "2023-06-10T20:13:08.430886Z"
    }
   },
   "outputs": [],
   "source": [
    "# X = filtered_df['text'].astype(str)\n",
    "# y = filtered_df['edInput']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Create an instance of CountVectorizer\n",
    "# count_vectorizer = CountVectorizer()\n",
    "\n",
    "# # Apply fit_transform to the training data\n",
    "# X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Apply transform to the testing data\n",
    "# X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# # Create an instance of LogisticRegression with increased max_iter\n",
    "# logistic_regression = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# # Train the logistic regression model using the training data\n",
    "# logistic_regression.fit(X_train_count, y_train)\n",
    "\n",
    "# # Make predictions on the testing data using CountVectorizer features\n",
    "# y_pred_count = logistic_regression.predict(X_test_count)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy_count = accuracy_score(y_test, y_pred_count)\n",
    "# precision_count = precision_score(y_test, y_pred_count)\n",
    "# recall_count = recall_score(y_test, y_pred_count)\n",
    "# f1_count = f1_score(y_test, y_pred_count)\n",
    "\n",
    "# print(\"CountVectorizer Accuracy: \", accuracy_count)\n",
    "# print(\"CountVectorizer Precision: \", precision_count)\n",
    "# print(\"CountVectorizer Recall: \", recall_count)\n",
    "# print(\"CountVectorizer F1-score: \", f1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8ab11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5309073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd894a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79b29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
